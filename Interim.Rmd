---
title: "Interim Report ST606"
author: "Niamh Harford 20251644"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
linestretch: 1.5
---
\pagenumbering{roman}
\newpage 

\tableofcontents

\newpage
# List of tables{-}
\renewcommand{\listtablename}{} <!-- removes default section name -->
\listoftables

\newpage
# List of figures{-}
\renewcommand{\listfigurename}{}
\listoffigures

<style type="text/css">
  body{
  font-size: 12pt;
}
h2,h3,h4,h6,h7,h8,h9,h10,h11,h12{
  font-size: 24pt;
}
h1,h5{
  font-size: 24pt;
}
</style>

\newpage
\pagenumbering{arabic}
# 1. Introduction{#intro}
<body>
The title of my thesis is "Predicting Hypertension". For this project I will be using data that is available from the the National Health and Nutrition Examination Survey (NHANES). I plan to examine which variables have the most significant contribution in causing hypertension. Further to this I hope to determine which model is the most appropriate model to predict hypertension. Studies like this can help establish leading causes of hypertension and thus can contribute to preventing extensive occurrences of hypertension cases in addition to potential knock-on effects that comprehensive hypertension can go on to cause, by offering useful advise in maintaining healthy blood pressure measures.

</body>


<body>
## 1.1. Hypertension{#hyper}

Results published in 1998 from the NHANES indicated that roughly 50 million adults in America were diagnosed with hypertension, which was defined to be a systolic blood pressure that is greater than 139 mm Hg or a diastolic blood pressure that is greater than 89 mm Hg (Kulkarni et al.,1998). Hypertension is an important condition that commonly leads to many serious illnesses including stroke and heart disease (Sakr et al., 2018), furthermore it is described as a common, chronic, age-related disorder, which entails severe cardiovascular and kidney complications (Staessen, Wang, Bianchi and Birkenhäger, 2003). It mainly arises as a complex quantitative characteristic that is affected by varying relationships of genetic and environmental factors, whereby these characteristics have turned out to be extremely harmful habits that have caused weakened heart function due to increased blood pressure (Staessen, Wang, Bianchi and Birkenhäger, 2003). There are many factors that may contribute to the permanent rise of blood pressure and it can more than often be a result of many factors working in combination with each other. Hypertension prevalence has been seen to rise with age, disregarding both the type of blood pressure measurement and the operative thresholds used for diagnosis (Staessen, Wang, Bianchi and Birkenhäger, 2003). There are many other characteristics to consider as contributing factors to hypertension as age increases. There are many habits for example exercise that as people get older, they may refrain from. The American Heart Association have stated that the estimates for hypertension prevalence are comparable between both men and women, are highest among African Americans where 44% of people surveyed that had hypertension were African American, and showed that only 53% of all hypertensive adults have their blood pressure under some control (Go et al., 2013). This statement highlights how the different racial groups may have an effect of the prevalence of hypertension, which association may not have been initially considered. An interesting statistic has revealed that hypertension is considered in the group of risk factors for cardiovascular disease that caused up to 17.7 million deaths around the world in 2015 alone (López-Martínez et al, 2020). This figure reveals how serious hypertension is to our health and the effect it can potentially have on all life if blood pressure measures are not monitored and controlled.
</body>  



<body>
## 1.2. NHANES Data{#Nhanes}

The data that I will use is from the National Health and Nutrition Examination Survey, which is more simply known as NHANES. NHANES is an application of research created to evaluate the physical and nutritional status of both children and adults in America (NHANES - About the National Health and Nutrition Examination Survey, 2021). This data is published online and can be publicly accessed for extensive health related statistical analysis. The survey is distinctive in that it uses both interviews and physical assessments to examine the health of the people who are being sampled. The NHANES was first established in the early 1960s and has been ran as a number of surveys focusing on different population groups or health topics (NHANES - About the National Health and Nutrition Examination Survey, 2021). In 1999, the survey became a sustained program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs (NHANES - About the National Health and Nutrition Examination Survey, 2021). NHANES surveys a nationally representative sample of about 5,000 persons annually (NHANES - About the National Health and Nutrition Examination Survey, 2021). The NHANES interview includes many components in particular demographic, socioeconomic, dietary, and health-related questions. The physical assessment component consists of medical, dental, and physiological measurements, as well as laboratory tests dispensed by highly trained medical personnel (NHANES - About the National Health and Nutrition Examination Survey, 2021). During the time of 1999–2010, the NHANES subject matter was altered biannually as parts were changed, added, or removed (NHANES - About the National Health and Nutrition Examination Survey, 2021). Changes were also made to focus on new age groups, alter data collection methods, and update technologies (NHANES - About the National Health and Nutrition Examination Survey, 2021). A complex, multi-step probability sampling plan was used to choose a sample representative sample of people in America (NHANES - About the National Health and Nutrition Examination Survey, 2021). Findings from surveys such as NHANES will be used to discover the prevalence of major diseases and risk factors for diseases (NHANES - About the National Health and Nutrition Examination Survey, 2021). 




## 1.3. Literature Review{#litreview}

<body>

I thoroughly examined previous papers published that used several models to predict hypertension. Some papers focused on adopting neural network models to make some inference on hypertension, whereas other papers centered their focus on logistic regression models, to both examine classification capabilities and accuracy measures. Additionally, there were papers that used both models mentioned above along with the random forest model. Major focal points in these papers included examining training and test sets, model error rate assessment and model accuracy. López-Martínez et al. paper shows that their artificial neural network model improved the overall performance and accuracy of a logistic regression model proposed in a previous paper that used the same input features as their paper (López-Martínez et al, 2020). The results showed that the artificial neural network was more fitting than the logistic regression to a small degree. This paper compared their results using an artificial neural network to those of a previous paper that only used a logistic regression to predict hypertension.

</body>
\newpage
# 2. Methods{#methods}

<body>

</body>

## 2.1. Setting Up the Data{#setup}

<body>

I chose to use the 2011/2012 data as my training set to fit the model. For the test set, to evaluate model fits, I chose to use another year group which is the 2015/2016 data. I obtained the variables that were appropriate for my analysis from the NHANES data, to acquire this data I used the 'nhanesA' package (Endres, 2021). Using 'nhanesA', I imported eight data files from the 2011-2012 NHANES data. Six of eighth files came from the questionnaire data (SMQ_G, DIQ_G, KIQ_G, BPQ_G, PAQ_G, ALQ_G), while the remaining two came from the dietary (DR1TOT_G) and examination data (BMX_G). SMQ_G contains data on smoking use, DIQ_G contains data on diabetes, KIQ_G contains data on kidney conditions, BPQ_G contains data on blood pressure and cholesterol, PAQ_G contains data on physical activity, ALQ_G contains information on alcohol use, DR1TOT_G contains data on total nutrition intake and BMX_G contains data on body measures. Along with these I also imported the data containing the demographics table (DEMO_G), which contained personal details about the candidate such as age and which ethic group they belonged to. I merged all the data together to form one data set, where I then choose eleven explanatory variables, in addition to one response variable. The explanatory variables from the NHANES data set are RIAGENDR, RIDAGEYR, RIDRETH1, SMQ020, DIQ010, KIQ022, BMXBMI, PAQ665, ALQ101, DRQSPREP and DR1TPOTA. The response variable is BPQ020, which corresponds to Hypertension. I repeated this process to obtain the same data for the 2015/2016 test set. Each variable contained a question where the participants response or measurements were recorded. Table 1 reveals a summary of what the variables were renamed for convenience and the description or question for each variable. 

 
```{r, include=TRUE, echo=FALSE, message=FALSE}
suppressMessages(library(kableExtra))
Data <- factor(rep(c("Questionaire","Examination", "Dietary","Demograpic"), c(6,1,2,3)))
Files <- factor(rep(c("BPQ_G","SMQ_G", "DIQ_G", "KIQ_G", "PAQ_G", "ALQ_G", "BMX_G", 
                      "DR1TOT_G", "DEMO_G"),c(1,1,1,1,1,1,1,2,3)))
Variables <- factor(c("BPQ020", "SMQ020", "DIQ010", "KIQ022", "PAQ665", "ALQ101", "BMXBMI", "DRQSPREP", "DR1TPOTA", "RIAGENDR", "RIDAGEYR", "RIDRETH1"))
Description <- factor(c("Have you ever been told that you have hypertension?",
                      "Have you smoked more that 100 cigarettes during your lifetime?","Have you ever been tld that you have diabetes?","Have you ever been told that you have weak or failing kidneys?", "In a normal week would you do any moderate-intensity sports?","In any one year, have you had at least 12 of any type of alcoholic drinks?","Body Mass Index (kg/m**2)","How often would you be likely to add salt to your food at the table?","Potassium (mg)", "Gender of participants", "Age in years, individuals who are 80 years of age and over are top-coded", "Race or ethnic group of participants"))
Renamed <- factor(c("Hypertension","Smoking","Diabetes","KidneyF","PhysActivity","Alcohol","BMI","Salt","Potassium", "Gender","Age","Race"))

data=data.frame(Variables,Renamed,Description)

kable(data, "latex", booktabs=TRUE, linesep="", caption="NHANES data\\label{tbl:data0}") %>%
    kable_styling(latex_options=c("HOLD_position"))



```

</body>

## 2.2. Data Cleaning{#dataclean}
<body>

I was required to clean the data, as quite often data obtained from surveys will include missing data. I discovered there were 1712 and 1453 NA values existing in the 2011/2012 and 2015/2016 data sets respectively. After fully examining the data, I determined the majority of NA values were present in the 'Don't Know' or 'Refused' levels within the variables, in which participants responded. Additionally, these two responses made interpretation quite difficult, therefore after some consideration I chose to remove these responses from the data. I then only omitted 52 and 45 NA values respective of the 2011/2012 and 2015/2016 data sets. I verifies categorical variables were notes as factors and continuous variables as numeric. I check all levels of the categorical variables and was satisfied the data was cleaned to a satisfactory degree.

</body>


## 2.3. Modelling{#model}

<body>

I choose to use three machine learning techniques to investigate and predict hypertension. These models included logistic regression, random forest and neural network models. I choose these models as I had the ability to use non-linear activation functions to eliminate the non-linearity of the input features. As the response variable is binary all models are quite appropriate for classification techniques.\

Logistic regression models is such a model that generally deals with binary outcomes (Hall and Round, 1994). This model will always produce probabilities between 0 and 1. Logistic regression produces odds ratios, these are the probability of the event occurring to the probability of it not occurring (Hall and Round, 1994).\

A random forest model (Prasad, Iverson,and Liaw, 2006; Ho, 1995) is a classification algorithm that operates by producing a large number of decision trees at both training and at testing (Sakr et al., 2018). Random forest models were created to produce accurate predictions that do not over-fit the data (Breiman 2001,
2002). Random forest models uses bootstrap samples to construct multiple trees, although each tree is created with a randomised subset of predictors (Prasad, Iverson,and Liaw, 2006). The quantity of trees utlised in random forest models is controlled by the user, the default number of trees used is 500.\

A neural network model is called a network as they can be associated with a directed acyclic graph describing how the functions are composed together. Neural network models are typically trained to treat the non-linearity of explanatory variables due to the non-linear nature of them, making the model more flexible compared to logistic regression (López-Martínez et al, 2020). Each node produced by neural networks are known as a unit. A neuron unit $u$ first takes a linear combination of the explanatory variables and applies a non-linear function $f$, called an activation function. **These are taken from your notes I need to add more and get references to this section**.

### 2.3.1 Logistic Regression{#logreg}

Initially, I fitted several logistic regression models. The equation for logistic regression is defined below:

$P(Y=1|X=x) = \frac{e^{\small \eta}}{1+e^{\small \eta}}$ , where ${\small \eta}$ is the systematic component of the model. Eg. ${\small \eta} = \beta_0 + \beta_1x_1 + . . . +\beta_kx_k$.\

I used the function residualPlots() from the  to examine how well each variable fit in the model. The first model I fit included all explanatory variables. Upon inspecting how well each variable fir, it was evident that two continuous variables age and BMI did not fit well. I decided to add two quadratic terms to the model on Age and BMI, while keeping the variables without this terms too. Using the function anova(), I was able to determine the model including quadratic terms was a better fit model. Following this I fit two more models to examine potential interactions with the Gender variable. The first of these models included an interaction with gender and all other variables, including the two quadratic terms. Using the function Anova(), I could see how both interactions with the quadratic terms were significant. This is where I fit the second model, with an interaction with gender and the following Age, Age^2^, BMI and BMI^2^.
</body>

### 2.3.2 Random Forest{#rf}

<body>
I fit a random forest model with importance equals true using the 'randomForest' package (Liaw and Wiener 2002). 

</body>


### 2.3.3 Neural Network{#nn}

<body>
I fit a neural network model with size equal to 10 using the 'nnet package' (Venables and Ripley, 2002). 
</body>


## 2.4. Tuning Models{#modeltune}

<body>
I will complete this section following further neural network and possible random forest tuning.
</body>


## 2.5. Comparing Models{#comparemodels}

<body>
 
I used several methods to compare the logistic regression, random forest and neural network models. The confusion matrix, to examine the error rate, ROC (Receiver Operating Characteristic) curve, AUC (Area Under the Curve), partial dependence plots / profiles and the 'condvis2' package (Hurley, O'Connell and Domijan, 2020) in R were all measures used to examine the accuracy of all models.\

Utilising the confusion matrix, I can produce measures of model accuracy and error rates to determine the fit of all models.

The ROC curve is the plot of sensitivity versus 1-specificity. This ROC curve evaluates the accuracy of models. The ROC curve reveals the trade off between the false positive rate (FPR) and true positive fraction (TPR) as one change the criterion for positivity (Swets, 1979; Metz, 1986). The FPR is also referred to as 1-specificity and the TPR is also referred ro as sensitivity. The AUC and ROC curve work hand in hand. The AUC is another technique to evaluate the accuracy of models. The ROC curve is a more visual representation of how accurate the model is, whereas the AUC is the measure of the area under the ROC curve and will produce a figure between 0 and 1, where an AUC value of 1 reveals a model is extremely accurate. The AUC value produces a summary of the entire location of the ROC curve instead of depending on a specific operating point (Swets, 1979; Hanley and McNeill, 1982).\

The partial dependence plots are utilised using the 'DALEX' package (Biecek, 2018). The main point of partial dependence profiles is to investigate how the expected value of model prediction will behave as a function of a selected explanatory variable (Biecek and Burzykowski, 2021). Partial dependence profiles are beneficial for comparisons of different models. Partial dependence profiles use the mean of ceteris-paribus profiles. Ceteris-paribus profiles displays the dependence of an instance-level prediction on any explanatory variable, therefore the partial dependence profile is estimated by the mean of the ceteris-paribus profiles for all observations from a data set (Biecek and Burzykowski, 2021). The equation for the partial dependence profile is defined below:

$\hat{g}^j _{PD} (z) = \frac{1}{n}$$\sum_{i=1}^{n}f(\underbar{x}_i^{j|=z})$\

The value of the partial dependence profile for model $f()$ and explanatory variables $\underbar{x}_i^{j}$ at $z$. $z$ is fixed over the marginal distribution of $\underbar{x}$.\

The final method I employed to compare the three models was the 'condvis2' package (Hurley, O'Connell and Domijan, 2020). Using this package I could examine several plots to see the predicted probability of the explanatory variables and potential interactions with other explanatory variables.

</body>

\newpage
# 3. Results{#results}

<body>


## 3.1 Logistic Regression{#resultslogreg}

Using logistic regression models, I was able to investigate potential increases and decreases in odds ratios for hypertension across different explanatory variables. Figure \ref{fig:hist1} displays all the odds ratios of each variable. The highest odds ratio evidently belongs to Gender[Female]. This figure interprets to, the odds of having hypertension is expected to be 7 times higher for females than for males. Several predictable results displayed in figure \ref{fig:hist1}}, include increased odds of having hypertension for individuals who have kidney disease, diabetes, borderline diabetes and individuals who smoke. All levels within the Race variable are seen to have increased odds of having hypertension. The highest odds ratio for these levels with Race, was for non-hispanic black individuals. For a one unit increase in Age and BMI measures the odds of having hypertension is seen to slightly decrease for females than for males. Being physically active is also seen to decrease the odds of having hypertension. The addition of salt to food prep at the table is not seen to have a significant effect on hypertension. An unexpected result displayed in figure \ref{fig:hist1}, is Salt[Very Often] is seen to decrease the odds of having hypertension. However, it is important to note figure \ref{fig:hist1} is not a great representation for the odds ratios of Age, Age^2^, BMI and BMI^2^ and the interaction terms associated with the quadratic terms on these variables (For the future I must look at how to properly dipslay these results and interpret them correctly). 



```{r, echo = F, results='hide', fig.height=4}
suppressMessages(library(sjPlot))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(gtable))
suppressMessages(library(ggplot2))
load("Data/data1_11a.RData") 

data_11a$Hypertension <-factor(ifelse(data_11a$Hypertension == "Yes", 1 ,0))

f4 <-glm(Hypertension ~ (Age+I(Age^2) + BMI+I(BMI^2))*Gender+Race + Smoking + Diabetes + KidneyF + PhysActivity + Alcohol + Salt + Potassium, data=data_11a, family = binomial())


set.seed(123)
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(DALEX))
suppressMessages(library(rms))
suppressMessages(library(nnet))
suppressMessages(library(neuralnet))
suppressMessages(library(devtools))
suppressMessages(source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'))


model_log1 <- f4

model_rf <- randomForest(Hypertension ~ .,data_11a, importance = T)

model_nn <- nnet(Hypertension ~ ., data=data_11a, size=10)

hypn <- as.numeric(data_11a$Hypertension)-1
ex_rf <- explain(model_rf, data = data_11a, y = hypn )
ex_log <- explain(model_log1, data = data_11a, y = hypn)
ex_nn <- explain(model_nn, data = data_11a, y = hypn)

```

``````{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.cap="The odds ratios for the logistic regression. The purple dots indicate increased odds ratios. The green dots indicate decreased odds ratios.\\label{fig:hist1}"}
plot_model(f4, vline.color = "black", sort.est = TRUE,colors = "Accent")
```

The partial dependence plot (figure \ref{fig:hist2}) for the logistic regression model reveals how the average prediction for hypertension continues to increase for a one unit increase in age for both males and females. The average prediction for hypertension continues to increase for a one unit increase in BMI for females, however, from figure \ref{fig:hist2} it is evident there is a sharp decrease in the prediction of hypertension for males when BMI reaches roughly 40. Potassium appears to have a steady, however minor decrease in the prediction for hypertension for both genders.

``````{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Partial dependence plot for the logistic regression response for age, bmi and potassium, showing the effect of gender\\label{fig:hist2}"}
plot(model_profile(ex_log,groups="Gender")) + ggtitle("Response for Logistic Regression") + theme(plot.title = element_text(size = 12))
```



## 3.2. Random Forest{#resultsrf}

Similarly,the partial dependence plot for the random forest model (figure \ref{fig:hist3}) reveals how the average prediction for hypertension continues to increase for a one unit increase in age for both males and females. The average prediction for hypertension here continues to increase for a one unit increase in BMI for both females and males. The increasing trend for BMI begins to level off around BMI of 50. Potassium appears to have a comparable trend to the logistic regression model, the predictions for hypertension for both genders level off quite rapidly.

``````{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Partial dependence plot for the random forest response for age, bmi and potassium, showing the effect of gender\\label{fig:hist3}"}
plot(model_profile(ex_rf,groups="Gender")) + ggtitle("Response for Random Forest") + theme(plot.title = element_text(size = 12))
```


## 3.3. Neural Network{#resultsnn}

The results for the neural network are not very informative. The partial dependence plots for the neural network model (figure \ref{fig:hist4}) all show the same pattern. Age, BMI and Potassium appear to have no trend as all lines appear to be leveled off for both genders. This model will need further improvement.

``````{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Partial dependence plot for the neural network response for age, bmi and potassium, showing the effect of gender\\label{fig:hist4}"}
plot(model_profile(ex_nn,groups="Gender")) + ggtitle("Response for Neural Network")+ theme(plot.title = element_text(size = 12))
```

## 3.4. Measures of Accuracy{#accuracy}

The random forest model was the most accurate model. This model produced an AUC value of 0.999. Figure \ref{fig:hist5} shows how well the random forest model is fit. The random forest model had 0.994 accuracy. The logistic regression model also performed well, it produced an AUC value of 0.818. The accuracy for the logistic regression was 0.747. The ROC curve shows the logistic regression model is quite satisfactory. The neural network model shows very little accuracy and appears to be a poor fit for the data. 

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=4, fig.cap="ROC curve for all models\\label{fig:hist5}"}
plot(model_performance(ex_rf),model_performance(ex_log),model_performance(ex_nn), geom = "roc")+ theme(plot.title = element_text(size = 12)) 
```

Table 2 displays the error rate for all models. The random forest model produced the lowest test error rate at 25.82%, with the logistic regression at 27.01%. The random forest model is the better fit model.
**(I cannot get the Neural Network error rate yet but I will)**

</body>

```{r, echo=FALSE, eval=FALSE, message=FALSE, warning=FALSE}

plot(model_profile(ex_rf, variables = c("Gender", "Race")))  + ggtitle("Response for Random Forest")
plot(model_profile(ex_rf, variables = c("Smoking", "Diabetes"))) + ggtitle("Response for Random Forest")
plot(model_profile(ex_rf, variables = c("KidneyF","PhysActivity", "Alcohol", "Salt"))) + ggtitle("Response for Random Forest")
plot(model_profile(ex_rf)) + ggtitle("Response for Random Forest")
plot(model_profile(ex_log, variables = c("Gender", "Race","Smoking", "Diabetes"))) + ggtitle("Response for Logistic Regression")
plot(model_profile(ex_log, variables = c("KidneyF","PhysActivity", "Alcohol", "Salt"))) + ggtitle("Response for Logistic Regression")
plot(model_profile(ex_log)) + ggtitle("Response for Logistic Regression")
plot(model_profile(ex_nn, variables = c("Gender", "Race","Smoking", "Diabetes"))) + ggtitle("Response for Neural Network")
plot(model_profile(ex_nn, variables = c("KidneyF","PhysActivity", "Alcohol", "Salt"))) + ggtitle("Response for Neural Network")
plot(model_profile(ex_nn)) + ggtitle("Response for Neural Network")



 


# look at model_performance and plot model_performance for nice way of getting roc and model summaries.

 

model_performance(ex_rf)$measures$auc
model_performance(ex_log)
model_performance(ex_rf)$measures$accuracy
model_performance(ex_log)$measures$accuracy
mod_nn <-model_performance(ex_nn)
mod_nn$measures$mad
```



```{r, echo = FALSE, eval=F}
p1<- plot_model(f4, type = "pred", terms = c("BMI"))
p2 <- plot_model(f4, type = "pred", terms = c("BMI","Gender"))
p3 <-plot_model(f4, type = "pred", terms = c("Age"))
p4 <- plot_model(f4, type = "pred", terms = c("Age","Gender"))

g2 <- ggplotGrob(p1)
g3 <- ggplotGrob(p3)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)


g2 <- ggplotGrob(p2)
g3 <- ggplotGrob(p4)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

p5 <- plot_model(f4, type = "pred", terms = c("Potassium"))
p6 <- plot_model(f4, type = "pred", terms = c("Smoking"))

g2 <- ggplotGrob(p5)
g3 <- ggplotGrob(p6)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

p7 <-plot_model(f4, type = "pred", terms = c("Diabetes"))
p8 <-plot_model(f4, type = "pred", terms = c("KidneyF"))

g2 <- ggplotGrob(p7)
g3 <- ggplotGrob(p8)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

p9 <-plot_model(f4, type = "pred", terms = c("Race"))
p10 <-plot_model(f4, type = "pred", terms = c("PhysActivity"))

g2 <- ggplotGrob(p9)
g3 <- ggplotGrob(p10)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

p11 <-plot_model(f4, type = "pred", terms = c("Alcohol"))
p12 <-plot_model(f4, type = "pred", terms = c("Salt"))

g2 <- ggplotGrob(p12)
g3 <- ggplotGrob(p11)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
grid.newpage()
grid.draw(g)

```


```{r, echo = F}
Model = factor(c("Logistic Regression","Random Forest", "Neural Network"))
Error_Rate = c(25.82,27.01,"NA atm")
data=data.frame(Model,Error_Rate)

kable(data, "latex", booktabs=TRUE, linesep="", caption="Test Error Rate\\label{tbl:data1}") %>%
    kable_styling(latex_options=c("HOLD_position"))
```




\newpage
# 4. Discussion{#diss}

<body>
This project investigated 'predicting hypertension' with the implementation of three machine learning models. Each model was examined to determine the most appropriate model in predicting hypertension. The findings are considered in 4.1. Model Selection and 4.2. Variable Importance.
</body>


## 4.1. Model Selection{#modsel}

<body>
Similar to a previous study (Sakr et al., 2018), the random forest model has outperformed the other models by showing the best overall performance (AUC = 0.999). The model proved to be extremely accurate in predicting hypertension (accuracy = 0.994). The logistic regression model was satisfactory (AUC = 0.818), however, it was not accurate as the random forest model (accuracy = 0.747). Currently, the neural network model does not appear to be at all satisfactory. 
</body>


## 4.2. Variable Importance{#varimport}

<body>

</body>


## 4.3. Future Work{#future}

<body>

My future plans for this project is to further train the neural network and random forest models and produce the best fit model I can for both of these models. I plan to further evaluate all model fits using the test set. Finally I will reach a conclusion as to which model will best predict hypertension and determine what of the variables used in these models are most responsible for contributing to the cause of hypertension.
</body>


# 4. Conclusion{#con}

<body>


https://www.sciencedirect.com/science/article/abs/pii/S095741741830352X?via%3Dihub
+ Best AUC 0.73 (95% CI [0.70–0.76]) showing fair result with the final diagnosis.
+ This model predicts correctly hypertensive individuals 73% better than a randomly selected individual.
+ According to the model Kidney disease and smoking habits do not affect odds of the outcome.
+ According to the model odds of having hypertension is higher for female individuals than for male.
+ Non-Hispanic black have higher odds of having hypertension than the rest of ethnics groups.

</body>

\pagebreak

# References{#ref}

<body>

1. Sakr, S., Elshawi, R., Ahmed, A., Qureshi, W., Brawner, C., Keteyian, S., Blaha, M. and Al-Mallah, M., 2018. Using machine learning on cardiorespiratory fitness data for predicting hypertension: The Henry Ford ExercIse Testing (FIT) Project. *PLOS ONE,* 13(4), p.e0195344.
2. Staessen, J., Wang, J., Bianchi, G. and Birkenhäger, W., 2003. Essential hypertension. *The Lancet*, 361(9369), pp.1629-1641.
3. World Health Organization. World Health Statistics 2017: *Monitoring Health for The SDGs.* arXiv:1011.1669v3 (2017).
4. Go, A.S., Mozaffarian, D., Roger, V.L., Benjamin, E.J., Berry, J.D., Borden, W.B., Bravata, D.M., Dai, S., Ford, E.S., Fox, C.S. and Franco, S., 2013. Heart disease and stroke statistics—2013 update: a report from the American Heart Association. *Circulation*, 127(1), pp.e6-e245.
5. López-Martínez, F., Núñez-Valdez, E., Crespo, R. and García-Díaz, V., 2020. An artificial neural network approach for predicting hypertension using NHANES data. *Scientific Reports*, 10(1).
6. Cdc.gov. 2021. NHANES - About the National Health and Nutrition Examination Survey. [online] Available at: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm> [Accessed 31 May 2021].
7. Kulkarni, S., O'Farrell, I., Erasi, M. and Kochar, M.S., 1998. Stress and hypertension. *WMJ: official publication of the State Medical Society of Wisconsin*, 97(11), pp.34-38.
8. Swets, J.A., 1979. ROC analysis applied to the evaluation of medical imaging techniques. *Investigative radiology*, 14(2), pp.109-121.
9. Metz, C.E., 1986. ROC methodology in radiologic imaging. *Investigative radiology*, 21(9), pp.720-733.
10. Hanley, J.A. and McNeil, B.J., 1982. The meaning and use of the area under a receiver operating characteristic (ROC) curve. *Radiology*, 143(1), pp.29-36.
11. Lüdecke D (2021). _sjPlot: Data Visualization for Statistics in Social Science_. R package version 2.8.7, <URL: https://CRAN.R-project.org/package=sjPlot>.
12. Biecek, P. and Burzykowski, T., 2021. *Explanatory model analysis: explore, explain, and examine predictive model*s. CRC Press.
12. Biecek P (2018). “DALEX: Explainers for Complex Predictive Models in R.” _Journal of Machine Learning Research_, *19*(84), 1-5. <URL: https://jmlr.org/papers/v19/18-416.html>.
13. Christopher J. Endres (2021). nhanesA: NHANES Data Retrieval. R package version 0.6.5.3. https://CRAN.R-project.org/package=nhanesA
14. Catherine Hurley, Mark O'Connell and Katarina Domijan (2020). condvis2: Interactive Conditional Visualization for Supervised and Unsupervised Models in Shiny. R package version 0.1.1. https://CRAN.R-project.org/package=condvis2
15. A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22.
16. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0.
17. Ho, T.K., 1995, August. Random decision forests. *In Proceedings of 3rd international conference on document analysis and recognition*, 1, pp. 278-282.
18. Prasad, A.M., Iverson, L.R. and Liaw, A., 2006. Newer classification and regression tree techniques: bagging and random forests for ecological prediction. *Ecosystems*, 9(2), pp.181-199.
19. Breiman, L., 2001. Random forests. *Machine learning*, 45(1), pp.5-32.
20 Breiman, L., 2002. Using models to infer mechanisms. *IMS Wald Lecture*, 2, pp.59-71.
21. Hall, G.H. and Round, A.P., 1994. Logistic regression—explanation and use. Journal of the Royal College of Physicians of London, 28(3), p.242.



</body>