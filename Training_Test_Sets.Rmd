---
title: "Training / Test Sets"
author: "Niamh Harford 20251644"
date: "6/8/2021"
output: html_document
---

```{r}
# Load in 2011/2012 data
load("~/Documents/Masters/Dissertation/Project_Git/Data/data1_11a.RData") 

# Load in 2015/2016 Data
load("~/Documents/Masters/Dissertation/Project_Git/Data/hyp1.RData") 
```


```{r}
# Load in necessary packages
suppressMessages(library(tidyverse))
suppressMessages(library(sjPlot))
suppressMessages(library(sjlabelled))
suppressMessages(library(sjmisc))
suppressMessages(library(randomForest))
suppressMessages(library(nnet))
```


**Things to note **

+ 2011/2012 Data set will be my training and validation set

+ 2015/2016 Data set will be my test set

+ Training set where I am fitting the model 

+ Validation set where I am comparing results of the fit 

+ Test set is used to evaluate a model fit in the form of the test error rate


**Setting up the data **

```{r}
# Training Data
s <- sample(nrow(data_11a), round(.75*nrow(data_11a))) #2011/2012 data
Train <- data_11a[s,]
  
#Test Data
Test <- hyp1 #2015/2016 Data

#OR should I have it so the test set has less observations
p <- sample(nrow(hyp1), round(.25*nrow(hyp1)))
Test <- hyp1[p,]
```



**Random Forest**

```{r}
set.seed(1)

f1 <- randomForest(Hypertension ~ ., data=Train, importance=TRUE)

# Training error
preda <- predict(f1, Train)
mean(preda != Train$Hypertension)*100 # training error
# Test error
predb <- predict(f1, Test)
mean(predb != Test$Hypertension)*100

f1


# suppressMessages(library(condvis2))
# fit1args <- list(ptrans=exp)
# condvis(Train, f3, sectionvars="Hypertension",
# predictArgs=list(fit1args))
```


**Neural Network**

```{r}
set.seed(123)

f2 <- nnet(Hypertension ~ ., data=Train, size=6)
f2

# Training Data

tab1 <- table(predict(f2, Train, type="class"), Train$Hypertension)
tab1

# NN training Error rate
(tab1[1]+tab1[4]) / sum(tab1) *100 # around 25-27%


# Specificity 
specificity <- (tab1[2,1]) / (tab1[2,1]+tab1[2,2])
specificity *100 # ~65%

# Type 1 error
type1_error <- 1 - specificity
type1_error

#Sensitivity
sensitivity <- (tab1[1,2]) / (tab1[1,1]+tab1[1,2])
sensitivity *100 # ~78%

# Type 1 error
type2_error <- 1 - sensitivity
type2_error

# Precision
precision <- (tab1[1,2])/(tab1[1,2]+tab1[2,2])
precision *100 # ~81%


# Test Data

tab1 <- table(predict(f2, Test,type="class"), Test$Hypertension)
tab1


# NN test Error rate
(tab1[1]+tab1[4]) / sum(tab1) *100 # around 26-27%


# Specificity 
specificity <- (tab1[2,1])/ (tab1[2,1]+tab1[2,2])
specificity *100 # 61%

# Type 1 error
type1_error <- 1 - specificity
type1_error

#Sensitivity
sensitivity <- (tab1[1,2]) / (tab1[1,1]+tab1[1,2])
sensitivity *100 # 80%

# Type 1 error
type2_error <- 1 - sensitivity
type2_error

# Precision
precision <- (tab1[1,2])/(tab1[1,2]+tab1[2,2])
precision *100 #78%

# Accuracy
accuracy <- (tab1[2,1]+tab1[1,2])/sum(tab1)
accuracy *100
```


**Logistic Regression 1**

```{r}
Train$Hypertension <-ifelse(Train$Hypertension == "Yes", 1 ,0)

f3 <- glm(Hypertension~.,family="binomial",
          data=Train)
pred1 <- predict(f3, Test, type="response")
pred1 <- factor(ifelse(pred1 < .5, "No", "Yes"))
tab1 <- table(Test$Hypertension, pred1)
tab1

# Logistic Regression 1 Error rate
(tab1[1]+tab1[4]) / sum(tab1) *100 # around 26-27%


# Specificity 
specificity <- (tab1[2,1])/ (tab1[2,1]+tab1[2,2])
specificity *100 # 81%

# Type 1 error
type1_error <- 1 - specificity
type1_error

#Sensitivity
sensitivity <- (tab1[1,2]) / (tab1[1,1]+tab1[1,2])
sensitivity *100 # 58%

# Type 1 error
type2_error <- 1 - sensitivity
type2_error

# Precision
precision <- (tab1[1,2])/(tab1[1,2]+tab1[2,2])
precision *100 #63%
```


**Logistic Regression 2 - with gender interaction**

```{r}
f4 <- glm(Hypertension~.*Gender,family="binomial",
          data=Train)
pred2 <- predict(f4, Test, type="response")
pred2 <- factor(ifelse(pred2 < .5, "No", "Yes"))
tab <- table(Test$Hypertension, pred2)
tab1

# Logistic Regression 2 Error rate
(tab1[1]+tab1[4]) / sum(tab1) *100 # around 26-27%


# Specificity 
specificity <- (tab1[2,1])/ (tab1[2,1]+tab1[2,2])
specificity *100 # 81%

# Type 1 error
type1_error <- 1 - specificity
type1_error

#Sensitivity
sensitivity <- (tab1[1,2]) / (tab1[1,1]+tab1[1,2])
sensitivity *100 # 58%

# Type 1 error
type2_error <- 1 - sensitivity
type2_error

# Precision
precision <- (tab1[1,2])/(tab1[1,2]+tab1[2,2])
precision *100 #63%
```

**ROC** in the class(5 and 5.3) lecture

```{r}
#ROC RandomForest
require(pROC)
rf.roc<-roc(Train$Hypertension,f1$votes[,2])
plot(rf.roc)
auc(rf.roc)
#https://stats.stackexchange.com/questions/188616/how-can-we-calculate-roc-auc-for-classification-algorithm-such-as-random-forest#:~:text=Although%20the%20randomForest%20package%20does,combination%20with%20the%20package%20pROC.&text=These%20votes%20roughly%20represent%20a,a%20ROC%20and%20AUC%20measure.


suppressMessages(library(ROCR))

#ROC NN
prob <- predict(f2,  Train)
pred <- prediction(prob, Train$Hypertension)
perf <- performance(pred,"tpr","fpr")
plot(perf)


#ROC Logistic Regression 1
prob <- predict(f3,  Train)
pred <- prediction(prob, Train$Hypertension)
perf <- performance(pred,"tpr","fpr")
plot(perf)

#ROC Logistic Regression 2
prob <- predict(f4,  Train)
pred <- prediction(prob, Train$Hypertension)
perf <- performance(pred,"tpr","fpr")
plot(perf)
```



