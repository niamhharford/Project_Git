---
title: "A machine learning approach on NHANES data for predicting hypertension"
author: "Niamh Harford 20251644"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document
linestretch: 1.5
---

<center>

![](logo.png){width=50%}

</center>


\pagenumbering{gobble}
\newpage 
\pagenumbering{roman}
# Declaration{.unlisted .unnumbered}

\newpage
# Abstract{.unlisted .unnumbered}

<body>
Hypertension is an extremely common, high risk condition where blood pressure measures are higher than normal measures. Hypertension can often be conducive to severe cardiovascular and renal complications. The effects of extensive hypertension may be detrimental on the health of individuals if high blood pressure measures are not controlled and carefully monitored.
There are multiple factors that can provide reasoning behind the cause of hypertension, although there may be no one cause, it is crucial to determine what of those factors have major influence on increasing blood pressure. This study assesses the performance of three machine learning techniques on predicting hypertension using data from NHANES. For each classification model, this study will evaluate the association between age, race, gender, smoking, diabetes, kidney disease, physical activity, alcohol use, salt use and potassium in hypertensive individuals.



</body>


\newpage

# Acknowlegements{.unlisted .unnumbered} 

<body>
I would like to express my most sincere gratitude to my supervisor Dr. Catherine Hurley for all the support and whose invaluable expertise guided me prosperously through the course of this project. Catherine's guidance and enthusiasm proved to be extremely motivational and pushed me to successfully complete this project. I would also like thank all of my friends and family who have supported me throughout this project.

</body>

\newpage

\tableofcontents

\newpage
# List of tables{.unlisted .unnumbered}
\renewcommand{\listtablename}{} <!-- removes default section name -->
\listoftables

\newpage
# List of figures{.unlisted .unnumbered}
\renewcommand{\listfigurename}{}
\listoffigures




\newpage
\pagenumbering{arabic}
# 1. Introduction{#intro}
<body>
Hypertension is a systolic blood pressure that is greater than 139 mm Hg or a diastolic blood pressure that is greater than 89 mm Hg (Kulkarni et al.,1998). Systolic blood pressure is the highest pressure in the arteries when the heart beats and fills the arteries and diastolic blood pressure is the lowest pressure in the arteries when the heart is relaxed between beats (Kaplan, 2010). Hypertension can be a severe condition. It is often described as a common, chronic, age-related condition, which entails severe cardiovascular and kidney complications including stroke, heart disease and kidney disease (Sakr et al., 2018; Staessen, Wang, Bianchi and Birkenhäger, 2003). It mainly arises as a complex quantitative characteristic that is affected by varying relationships of genetic and environmental factors, whereby these characteristics have turned out to be extremely harmful habits that have caused weakened heart function due to increased blood pressure (Staessen, Wang, Bianchi and Birkenhäger, 2003).\  


There are many things to consider when investigating potential contributing factors of hypertension. It is important to note that in many cases there may be no one specific cause of hypertension and it can more than often be a result of many factors working in combination with each other (Kaplan, 2010). Hypertension prevalence has been seen to rise with age, disregarding both the type of blood pressure measurements and the operative thresholds used for diagnosis (Staessen, Wang, Bianchi and Birkenhäger, 2003). Additional factors such as race, smoking, certain chronic conditions like diabetes and kidney disease, obesity, physical inactivity, high salt intake, extensive alcohol use and low potassium levels can all contribute to the cause of hypertension (Kaplan, 2010). Some of these factors may be a consequence as individuals age. There are many habits for example physical activity, that as people get older they may refrain from doing. Smoking has been seen to increase the risk of hypertension by up to 3 times, as a consequence of increased risk in vascular damage due to intense surges in the arterial pressure (Sleight, 1993). This is no surprise as smoking often causes a temporary immediate increase in blood pressure in addition to many other extreme effects on the body. Hypertension has had an impact on approximately 70% of patients diagnosed with diabetes and is approximately two times more common in individuals with diabetes than in those without diabetes (Lago, Singh and Nesto, 2007). Additionally, chronic kidney disease is known to be both a common cause of hypertension and also a complication of unmanaged hypertension (Hamrahian and Falkner, 2016). An important factor to consider is the interaction between kidney disease and hypertension is complicated and can increase the risk of adverse cardiovascular and cerebrovascular issues (Hamrahian and Falkner, 2016). Nowadays, obesity is a general problem in much of the western world and it is widely known to be linked directly with multiple diseases and conditions including hypertension (Aneja, El-Atat, McFarlane and Sowers, 2004). It is recognised that fat cells in our bodies are not merely an immobile organ for storing energy but they also secrete a host of factors that interact with each other and can bring about elevated blood pressure measures (Aneja, El-Atat, McFarlane and Sowers, 2004). In particular, the assumed role of leptin in the causation of hypertension through an activation of the sympathetic nervous system and a direct consequence on the kidneys, resulting in increased salt reabsorption leading to hypertension (Aneja, El-Atat, McFarlane and Sowers, 2004). This shows some indication of the effect of kidney complications, possible physical inactivity as a consequence of obesity and high salt levels in the body all have on influencing hypertension. Finally, not consuming enough potassium in your diet and consuming excessive amounts of sodium can raise blood pressure measures, thus giving rise to hypertension (The Role of Potassium and Sodium in Your Diet, 2021).\ 


Results published in 1998 from the National Health and Nutrition Examination Survey indicated that roughly 50 million adults in America were diagnosed with hypertension (Kulkarni et al., 1998). This statistic from one country alone exposes the severity of hypertension. The American Heart Association have stated that the estimates for hypertension prevalence comparable between both men and women, were seen to be highest among African Americans where 44% of people surveyed that had hypertension were African American, and showed that only 53% of all hypertensive adults have their blood pressure under some control (Go et al., 2013). This statement highlights how different ethnic groups may have atering effects on the prevalence of hypertension, which relationship may not have been initially considered. An interesting statistic has revealed that hypertension is considered in the group of risk factors for cardiovascular disease that caused up to 17.7 million deaths around the world in 2015 alone (López-Martínez et al, 2020). This figure reveals how serious hypertension is to our health and the effect it can potentially have on all life if blood pressure measures are not monitored and controlled.\ 

Machine learning (Alpaydin, 2020; Marsland, 2015) is a contemporary data analysis technique with the distinctive ability to learn and ameliorate its performance without being obviously programmed and without user instruction (Sakr et al., 2018). The principal objective of supervised machine learning classification algorithms is to provide an explanation of the dependent variable with regard to the independent variables (Sakr et al., 2018). Machine learning techniques are commonly used to predict models and nowadays, in medical fields there is an extensive use of machine learning based techniques, however, quite often highly trained medical personnel find it difficult to depend on such models in practice for numerous reasons (Kononenko, 2001; Deo, 2015; Obermeyer and Emanuel, 2016; Darcy, Louie and Roberts, 2016; Elshawi, Al-Mallah and Sakr, 2019). There is several explanations as to why this is the case. Firstly, the majority of accessible predictive models often focus on certain diseases and reply upon the knowledge of highly trained medical personnel (Singal, 2013; He, Mathews, Kalloo and Hutfless, 2014; Pederson, et al, 2016; Elshawi, Al-Mallah and Sakr, 2019). Applying predictive models on immense health information systems perhaps will not perform efficiently as a result of the availability of multiple, elaborate data sources and the diverse mixture of both patients and diagnoses (Elshawi, Al-Mallah and Sakr, 2019). Secondly, the majority of models that are developed by data analysts mainly focus on the prediction accuracy as a performance metric but rarely produce a meaningful explanations of their prediction (Futoma, Morris and Lucas, 2015; Elshawi, Al-Mallah and Sakr, 2019). However, there has been an increase in analysis towards predicting medical conditions and diseases despite these reasons.\ 

Recently, various studies have been using models to predict hypertension (Chang, Wang and Jiang, 2011; Abdullah, Zakaria and Mohamad, 2011; Farran,Channanath, Behbehani and Thanaraj, 2013; Elshawi, Al-Mallah and Sakr, 2019). The emphasis of these studies are to determine the best fitting model in predicting hypertension. Many papers focused on adopting multiple models using machine learning techniques which included examining neural network models, logistic regression models and random forest models to make some inference on hypertension. These papers focused on using training and test sets and assessing model error rates and model accuracy to determine the best machine learning approach to predict hypertension. One study solely implemented an artificial neural network model and carried out a thorough analysis comparing previous studies using alternative models (López-Martínez et al., 2020). This study revealed their artificial neural network model had an improved overall performance and accuracy in contract to a previous logistic regression model they had worked on and published (López-Martínez et al., 2020, 2018). Both of these models used NHANES data in addition to examining the same explanatory variables. The artificial neural network model produced AUC of 0.77, whereas the logistic regression model produced AUC of 0.73. From the comparison of these two models analsyed by López-Martínez et al. (2018,2020), it is evident that an artificial neural network perhaps dependent on explanatory variables is a moderately better model to predict hypertension. Another study assessed six machine learning techniques to establish the most fitting model to predict hypertension, these models were logitBoost, bayesian network classifier, locally weighted naive bayes, artificial neural network, support vector machine and random forest (Sakr et al., 2018). This analysis revealed that the random forest model outperformed all other models and produced AUC of 0.93. Although this study did not examine the logistic regression model like the previous papers, it is evident that the random forest model when compared with multiple machine learning techniques including the artificial neural network model is the most fitting model utilised here.\ 


Predicting hypertension can help establish leading causes of hypertension. Furthermore, using models to predict hypertension can contribute to preventing extensive occurrences of hypertension cases in addition to potential knock-on effects of hypertension, by offering useful advise in maintaining healthy blood pressure measures. I plan to build a model that will provide practical and beneficial information regarding hypertension. I will employ three machine learning techniques (logistic regression, random forest and neural network) to determine which model is the most appropriate model in predicting hypertension. I will determine significant contributing factors of hypertension by exploring the effect of multiple variables on hypertension. These variables are age, gender, race, smoking, diabetes, kidney disease, BMI, physical activity, alcohol use, salt use and potassium levels.\ 

I will use data from the National Health and Nutrition Examination Survey, which is more simply known as NHANES. NHANES is an application of research created to evaluate the physical and nutritional status of both children and adults in America (NHANES - About the National Health and Nutrition Examination Survey, 2021). This data is published online and can be publicly accessed for extensive health related statistical analysis. The survey is distinctive in that it uses both interviews and physical assessments to examine the health of the particoants who are being sampled. The NHANES was first established in the early 1960s and has been ran as a number of surveys focusing on different population groups or health topics (NHANES - About the National Health and Nutrition Examination Survey, 2021). In 1999, the survey became a sustained program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs (NHANES - About the National Health and Nutrition Examination Survey, 2021). NHANES surveys a nationally representative sample of about 5,000 persons annually (NHANES - About the National Health and Nutrition Examination Survey, 2021). The NHANES interview includes many components in particular demographic, socioeconomic, dietary, and health-related questions. The physical assessment component consists of medical, dental, and physiological measurements, as well as laboratory tests dispensed by highly trained medical personnel (NHANES - About the National Health and Nutrition Examination Survey, 2021). During the period of 1999 to 2010, the NHANES subject matter was altered biannually as parts were changed, added, or removed (NHANES - About the National Health and Nutrition Examination Survey, 2021). Changes were also made to focus on new age groups, alter data collection methods, and update technologies (NHANES - About the National Health and Nutrition Examination Survey, 2021). A complex, multi-step probability sampling plan was used to choose a sample representative sample of people in America (NHANES - About the National Health and Nutrition Examination Survey, 2021). Findings from surveys such as NHANES are  used to discover the prevalence of major diseases and risk factors for diseases (NHANES - About the National Health and Nutrition Examination Survey, 2021).\ 




Machine learning techniques currently have been well suited for analysing medical data. - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.184&rep=rep1&type=pdf
+ Kononenko, I., 2001. Machine learning for medical diagnosis: history, state of the art and perspective. Artificial Intelligence in medicine, 23(1), pp.89-109.

</body>
\newpage
# 2. Methods{#methods}

<body>

</body>

## 2.1. Setting Up the Data{#setup}

<body>

I used the 2011/2012 data as my training set to fit the model. For the test set to evaluate model fits, I used another year group which was the 2015/2016 data. I explored potential variables for my analysis from the NHANES data. There were several variables that I initially chose for the data, that I discovered had a large number of missing values present. After analysing all variables to determine their appropriateness to this analysis, I choose eleven explanatory variables. I acquired this data by using the 'nhanesA' package (Endres, 2021). Using 'nhanesA', I imported eight data files from the 2011-2012 NHANES data. Six out of the eight files came from the questionnaire data (SMQ_G, DIQ_G, KIQ_G, BPQ_G, PAQ_G, ALQ_G), the remaining two came from the dietary (DR1TOT_G) and examination data (BMX_G). SMQ_G contains data on smoking use, DIQ_G contains data on diabetes, KIQ_G contains data on kidney conditions, BPQ_G contains data on blood pressure and cholesterol, PAQ_G contains data on physical activity, ALQ_G contains information on alcohol use, DR1TOT_G contains data on total nutrition intake and BMX_G contains data on body measures. Along with these I also imported the data containing the demographics table (DEMO_G), which contained personal details about the candidate such as age and which ethic group they belonged to. I merged all the files together to form one data set, where I then choose eleven explanatory variables, in addition to one response variable. These explanatory variables from the NHANES data set are RIAGENDR, RIDAGEYR, RIDRETH1, SMQ020, DIQ010, KIQ022, BMXBMI, PAQ665, ALQ101, DRQSPREP and DR1TPOTA. The response variable is BPQ020, which corresponds to Hypertension. I repeated this process to gather the same data for the 2015/2016 test set. Each variable contained a question where the participants response or measurements were recorded. Table 1 reveals a summary of what the variables were renamed and the description or question corresponding to each variable. The entire 2011/2012 training data set had 5319 observations and the 2015/2016 test data set contained 5473 observations prior to data cleaning. It was necessary for me to translate the variables using the function nhanesTranslate(). This meant that any categorical variables that were coded with values like 0 and 1 would be translated to responses participants filled out, for example "No and "Yes". This process allowed for a straightforward interpretation of all categorical variables. 

</body>
 
```{r, include=TRUE, echo=FALSE, message=FALSE}
suppressMessages(library(kableExtra))
Data <- factor(rep(c("Questionaire","Examination", "Dietary","Demograpic"), c(6,1,2,3)))
Files <- factor(rep(c("BPQ_G","SMQ_G", "DIQ_G", "KIQ_G", "PAQ_G", "ALQ_G", "BMX_G", 
                      "DR1TOT_G", "DEMO_G"),c(1,1,1,1,1,1,1,2,3)))
Variables <- factor(c("BPQ020", "SMQ020", "DIQ010", "KIQ022", "PAQ665", "ALQ101", "BMXBMI", "DRQSPREP", "DR1TPOTA", "RIAGENDR", "RIDAGEYR", "RIDRETH1"))
Description <- factor(c("Have you ever been told that you have hypertension?",
                      "Have you smoked more that 100 cigarettes during your lifetime?","Have you ever been told that you have diabetes?","Have you ever been told that you have weak or failing kidneys?", "In a normal week would you do any moderate-intensity sports?","In any one year, have you had at least 12 of any type of alcoholic drinks?","Body Mass Index (kg/m**2)","How often would you be likely to add salt to your food at the table?","Potassium (mg)", "Gender of participants", "Age in years, individuals who are 80 years of age and over are top-coded", "Race or ethnic group of participants"))
Renamed <- factor(c("Hypertension","Smoking","Diabetes","KidneyF","PhysActivity","Alcohol","BMI","Salt","Potassium", "Gender","Age","Race"))

data=data.frame(Variables,Renamed,Description)

kable(data, "latex", booktabs=TRUE, linesep="", caption="NHANES data\\label{tbl:data0}") %>%
    kable_styling(latex_options=c("HOLD_position"))

```




## 2.2. Data Exploration and Cleaning {#explor}

<body>
To explore both the training and test data I used data visualisation techniques to examine whether any patterns or relationships were present among the variables. I plotted each explanatory variable with the response variable to assess if such patterns were present. After exploring the plots I observed several issues with the data. I first noticed there were a significant amount of missing values present in the data (figure \ref{fig:hist0}). There were a total of 1712 missing values in the 2011/2012 training data set which were present in 903 rows and 1453 missing values in the 2015/2016 test data set which were present in 754 rows. This quantity of missing values is a considerably large proportion of both training and test data sets. I explored options of dropping variables, deleting rows with missing values and imputing the missing values. I did employ the method of dropping variables in the earliest stages of variable selection, however at this phase the variables chosen were all suitable for the analysis so this was not a realistic option. It was evident 3 variables in particular stood out as having the greatest amount of missing values. These variables were alcohol, potassium and salt. I decided to further explore the data before disregarding the missing values.


</body>

\pagebreak

```{r, echo =FALSE}
suppressMessages(library(naniar))
suppressMessages(library(tidyverse))
load("Data/all_dat.RData")
load("Data/data_11.RData")


x <- gg_miss_var(data_11) + theme_bw() + labs(title ="2011/2012 Data") #All data 2011/2012

y <- gg_miss_var(all_dat) + theme_bw() + labs(title ="2015/2016 Data") #All data 2015/2016

suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(gtable))
g2 <- ggplotGrob(x)
g3 <- ggplotGrob(y)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)
```

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.cap="Missing values\\label{fig:hist0}"}
plot(g)
```

    
I inspected the levels of the categorical variables. I noticed there were two levels present in some of the categorical variables. These levels were "Don't Know" and "Refused". Participants would have had the opportunity to respond with these such responses during the survey. Through further assessment, it was evident there was either no observations or very little observations in these levels. Thus these levels did not provide any useful information for modelling and also made interpretation difficult. Figure \ref{fig:smoke} and figure \ref{fig:alco1} displays these levels and also highlights the presence of missing values in the data. After some consideration, the decision was made to delete the observations where participants responded either "Don't Know" or "Refused". Following the removal of these levels I omitted the remaining missing values from both the training and test sets. I omitted 52 missing values from the 2011/2012 training data set and 45 missing values from the the 2015/2016 test data set. Sequentially, I verified categorical variables were marked in R as factors and continuous variables as numeric. After all data cleaning, the training set now had a total of 4312 observations and the test set had a total of 4624 observations. I choose to control all categorical variables in such a way that they were positively associated with the response variable Hypertension. In the training data, the response variable had a total of 1576 individuals diagnosed with hypertension and 2376 individuals who did not have hypertension. In the test data, the response variable had a total of 1711 individuals diagnosed with hypertension and 2913 individuals who did not have hypertension. The final step I performed involved releveling the categorical variables using the relevel() function.\ 


</body>


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Bar chart displaying the effect of smoking on hypertension on the training data set.\\label{fig:smoke}",fig.show='hold'}
ggplot(data_11) +
      geom_bar(aes(x = Smoking, fill = Hypertension),position = "dodge") + xlab("Smoking") + labs(fill= "Hypertension")
#in both cases where people smoke or do not smoke, the proportion of people that do not have hypertension is higher than those who do have it
```

$\vspace{1.5cm}$

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Bar chart displaying the effect of alcohol on hypertension on the training data set.\\label{fig:alco1}",fig.show='hold'}
ggplot(data_11) +
      geom_bar(aes(x = Alcohol, fill = Hypertension),position = "dodge") + xlab("Smoking") + labs(fill= "Hypertension")
#in both cases where people smoke or do not smoke, the proportion of people that do not have hypertension is higher than those who do have it
```




## 2.4. Modelling{#model}

<body>

I choose to use three machine learning techniques to investigate and predict hypertension. These modelling techniques are logistic regression, random forest and neural network models. The response variable is binary taking on either "Yes" or "No" responses. The response variable was re-coded to take on values of either 0 or 1.\ 

### 2.4.1 Logistic Regression{#logreg}


Logistic regression models are such models that generally deal with binary dependent variables (Hall and Round, 1994). This model will always produce probabilities between 0 and 1. Odds ratios which are the ratios of the probability of the event occurring to the probability of the event not occurring, can be utilised for logistic regression models (Hall and Round, 1994).The equation for logistic regression is defined below:

\begin{center}
\[P(Y=1|X=x) = \dfrac{e^{\small \eta}}{1+e^{\small \eta}}\]\ 
\end{center}

Where ${\small \eta}$ is the systematic component of the model, eg. ${\small \eta} = \beta_0 + \beta_1x_1 + . . . +\beta_kx_k$. I fitted several logistic regression models to determine the most satisfactory model. I used the function residualPlots() from the 'car' package to examine how well each variable fit in the model (Fox and Sanford, 2019). The first model I fit included all explanatory variables. I inspected the fit of all variables and it was clear that two continuous variables Age and BMI did not fit well. I decided to add two quadratic terms to the model on Age and BMI, while keeping all variables in the model. However, the residualPlots() function here was not effective in portraying the significant difference between the two models. Instead using the function anova(), I was able to determine the model including quadratic terms was a better fit model. I examined potential interactions with the Gender variable. I fit a model where Gender had an interaction with all other variables, including the two quadratic terms. Using the function Anova(), I could see how both interactions with the quadratic terms (Age^2^ and BMI^2^) and Gender were significant. I then fit a model, where Gender had an interaction with Age, Age^2^, BMI and BMI^2^. I used anova() to determine the best model. The final model I fit was identical to the previous model, however I added one more interaction term with Age and Diabetes. Using anova() it was clear that this model was the most satisfactory model. The final model chosen for the logistic regression model is defined below:

\begin{center}

log(Hypertension) $= \beta_0 + \beta_1$(Age $  \times $ Gender) $+ \beta_2$(Age$^{2}   \times$ Gender) $+ \beta_3$(BMI $  \times$ Gender) $+ \beta_4$(BMI$^{2}   \times$ Gender) $+ \beta_5$(Race) $ + \beta_6$(Smoking) $+ \beta_7$(Diabetes) $+ \beta_8$(Diabetes $\times$ Age) $+ \beta_9$(KidneyF) $+ \beta_{10}$(PhysActivity) $+ \beta_{11}$(Alcohol) $+ \beta_{12}$(Salt) $+ \beta_{13}$(Potassium) 

\end{center}


### 2.4.2 Random Forest{#rf}

A random forest model (Prasad, Iverson and Liaw, 2006; Ho, 1995) is a classification algorithm that operates by producing a large number of decision trees at training (Sakr et al., 2018). Random forest models were created to generate accurate predictions for a model that do not over-fit the data (Breiman 2001, 2002). Random forest models use bootstrap samples to construct multiple trees, although each tree is created with a randomised subset of predictors (Prasad, Iverson and Liaw, 2006). The quantity of trees utilised in random forest models is controlled by the user, the default number of trees used is 500. I fit a random forest model with importance equals true using the 'randomForest' package (Liaw and Wiener 2002).\ 


### 2.4.3 Neural Network{#nn}

A neural network model is called a network as they can be associated with a directed acyclic graph describing how the functions are composed together. Neural network models are typically trained to treat the non-linearity of explanatory variables due to the non-linear nature of them, making the model more flexible compared to logistic regression (López-Martínez et al, 2020). Each node produced by neural networks are known as a unit. A neuron unit $u$ first takes a linear combination of the explanatory variables and applies a non-linear function $f$, called an activation function. I fit a neural network model with size = 1 and decay = 0.5 using the 'nnet package' (Venables and Ripley, 2002). 

</body>


## 2.5. Tuning Models{#modeltune}

<body>
I tuned the neural network model using the package 'caret' (Kuhn, 2021). I used a grid search approach to determine what size and decay value was most appropriate to produce the best neural network model. The metric utilised to determine the best model was the ROC. 
</body>


## 2.6. Comparing Models{#comparemodels}

<body>
 
I used several methods to compare the logistic regression, random forest and neural network models. These methods included examining the confusion matrix, to estimate the error rate, Receiver Operating Characteristic (ROC) curve, the Area Under the Curve (AUC) value, partial dependence plots and profiles and the 'condvis2' package (Hurley, O'Connell and Domijan, 2020) in R. All measures were used to examine the accuracy of all models. Utilising the confusion matrix, I can produce measures of model accuracy and error rates to determine the fit of all models.\

This ROC curve evaluates the accuracy of models. This curve is the plot of sensitivity versus 1-specificity. It reveals the trade off between the false positive rate (FPR) and true positive fraction (TPR) as one change the criterion for positivity (Swets, 1979; Metz, 1986). The FPR is also referred to as 1-specificity and the TPR is also referred to as sensitivity. The AUC and ROC curve work hand in hand. The AUC is another technique to evaluate the accuracy of models. The ROC curve is a more visual representation of how accurate the model is, whereas the AUC is the measure of the area under the ROC curve and will produce a figure between 0 and 1, where an AUC value of 1 reveals a model is extremely accurate. The AUC value produces a summary of the entire location of the ROC curve instead of depending on a specific operating point (Swets, 1979; Hanley and McNeill, 1982).\

The partial dependence plots are produced using the 'DALEX' package (Biecek, 2018). The main point of partial dependence profiles is to investigate how the expected value of model prediction will behave as a function of a selected explanatory variable (Biecek and Burzykowski, 2021). Partial dependence profiles are beneficial for comparisons of different models. Partial dependence profiles use the mean of ceteris-paribus profiles (Biecek and Burzykowski, 2021). Ceteris-paribus profiles display the dependence of an instance-level prediction on any explanatory variable, therefore the partial dependence profile is estimated by the mean of the ceteris-paribus profiles for all observations from a data set (Biecek and Burzykowski, 2021). The value of the partial dependence profile for model $f()$ and explanatory variables $\underbar{x}_i^{j}$ at $z$, where $z$ is fixed over the marginal distribution of $\underbar{x}$ is defined below:

\begin{center}
\newcommand\munderbar[1]{%
  \underaccent{\bar}{#1}}

\[\hat{g}^j _{PD} (z) = \dfrac{1}{n}\sum_{i=1}^{n}f(\underbar{x}_i^{j|=z})\]


\end{center}
  

The final method I employed to compare the three models was the use of the condvis() function. Using this function I could examine several plots to see the predicted probability of the explanatory variables and potential interactions with other explanatory variables.

</body>

\newpage
# 3. Results{#results}

<body>


## 3.1 Logistic Regression{#resultslogreg}

The logistic regression model chosen for fitting the model included two quadratic terms on the Age and BMI variable, in addition to an interaction with Gender on Age, Age^2^, BMI and BMI^2^ and an interaction with Age and Diabetes. Using this logistic regression model, I investigated potential increases and decreases in odds ratios for hypertension across different explanatory variables. Figure \ref{fig:hist1} displays all the odds ratios for each variable. The odds ratios of all explanatory variables that did not have an interaction with another variable are summarised in table \ref{tbl:data3}.\ 



```{r, echo = F, results='hide',message=FALSE, warning=FALSE}
suppressMessages(library(sjPlot))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(gtable))
suppressMessages(library(ggplot2))
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(DALEX))
suppressMessages(library(rms))
suppressMessages(library(nnet))
suppressMessages(library(neuralnet))
suppressMessages(library(devtools))
suppressMessages(source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'))
load("Data/data1_11a.RData")

data_11a$Hypertension <-factor(ifelse(data_11a$Hypertension == "Yes", 1 ,0))

# f4 <-glm(Hypertension ~ (Age+I(Age^2) + BMI+I(BMI^2))*Gender+Race + Smoking + Diabetes + KidneyF + PhysActivity + Alcohol + Salt + Potassium, data=data_11a, family = binomial())


set.seed(123)



model_log1 <- glm(Hypertension ~ (Age+I(Age^2) + BMI+I(BMI^2))*Gender+Race + Smoking + Diabetes + (Diabetes*Age) + KidneyF + PhysActivity + Alcohol + Salt + Potassium, data=data_11a, family = binomial())

model_rf <- randomForest(Hypertension ~ .,data_11a,importance = T)

model_nn <- nnet(Hypertension ~ ., data_11a, size = 1, decay = 0.5)

# load("Data/model_log1.RData")
# load("Data/model_rf.RData")
# load("Data/model_nn.RData")

hypn <- as.numeric(data_11a$Hypertension)-1
ex_rf <- DALEX::explain(model_rf, data = data_11a, y = hypn, type = "classification")
ex_log <- DALEX::explain(model_log1, data = data_11a, y = hypn, type = "classification")
ex_nn <- DALEX::explain(model_nn, data = data_11a, y = hypn, type = "classification")



load("Data/hyp1.RData")
Test <- hyp1
Test$Hypertension <-factor(ifelse(Test$Hypertension == "Yes", 1 ,0))

hypn1 <- as.numeric(Test$Hypertension)-1
ex_log_test <- DALEX::explain(model_log1, data = Test, y = hypn1, type = "classification")
ex_rf_test <- DALEX::explain(model_rf, data = Test, y = hypn1, type = "classification")
ex_nn_test <-  DALEX::explain(model_nn, data = Test, y = hypn1, type = "classification")


Race_hispanic <- summary(model_log1)$coefficients[7,1] %>% exp() %>% round(digits = 3)
Race_white <- summary(model_log1)$coefficients[8,1] %>% exp() %>% round(digits = 3)
Race_black <- summary(model_log1)$coefficients[9,1] %>% exp() %>% round(digits = 3)
Race_other <- summary(model_log1)$coefficients[10,1] %>% exp() %>% round(digits = 3)
Smoke <- summary(model_log1)$coefficients[11,1] %>% exp() %>% round(digits = 3)

Kidn <- summary(model_log1)$coefficients[14,1] %>% exp() %>% round(digits = 3)
pactivity <- summary(model_log1)$coefficients[15,1] %>% exp() %>% round(digits = 3)
alco <- summary(model_log1)$coefficients[16,1] %>% exp() %>% round(digits = 3)
salt_rarely <- summary(model_log1)$coefficients[17,1] %>% exp() %>% round(digits = 3)
salt_occ <- summary(model_log1)$coefficients[18,1] %>% exp() %>% round(digits = 3)
salt_voften <- summary(model_log1)$coefficients[19,1] %>% exp() %>% round(digits = 3)
pota <- summary(model_log1)$coefficients[20,1] %>% exp() %>% round(digits = 3)
```


```{r, echo = FALSE, include=TRUE,message=FALSE}

Var = factor(c("Race - Other Hispanic","Race - Non-Hispanic White", "Race - Non-Hispanic Black", "Race - Other Race","Smoking - Yes", "Kidney Disease - Yes","Physical Activity - Yes", "Alcohol - Yes", "Salt - Rarely", "Salt - Occasionally", "Salt - Very Often", "Potassium"))
Odds_Ratio = c(Race_hispanic,Race_white,Race_black,Race_other,Smoke,Kidn,pactivity,alco,salt_rarely,salt_occ,salt_voften,pota)
data3=data.frame(Var,Odds_Ratio)

kable(data3, "latex", col.names =c("Variables","Odds Ratios"), booktabs=TRUE, linesep="", caption="Odds ratios for logistic regression model\\label{tbl:data3}") %>%
    kable_styling(latex_options=c("HOLD_position"))
```




Figure  \ref{fig:hist1} also displays the comparative odds ratios for all explanatory variables. The logistic regression model revealed some significant results, with some of these odds ratios being somewhat expected. Individuals who smoked more than 100 cigarettes in their lifetime had a 26% increase in their odds of having hypertension in comparison to individuals who have not smoked more than 100 cigarettes. For individuals who were diagnosed with kidney disease or failing kidneys, the odds of them having hypertension were estimated to be almost 3 times higher than those who did not have such renal complications. For individuals who were physically active, their odds of having hypertension were estimated to decrease by 10%. All levels within the Race variable are seen to have increased odds of having hypertension relative to the baseline race, Mexican American. The highest odds ratio within Race was for non-Hispanic black individuals. When you move from individuals who are American Mexican to Non-Hispanic Black, the odds of having hypertension is estimated to be 2.3 times higher. An unexpected result included the odds ratios for some levels of salt relative to the baseline "Never". The extensive use of salt to food at the table is not seen to have a significant increase on the effect of hypertension. Salt used "Very Often" is seen to decrease the odds of having hypertension by 2%, where as salt used "Occasionally" is considered to have no real change in the odds of having hypertension, both levels relative to "Never" using salt. Additionally, salt used "Rarely" compared to "Never", does decrease the estimated odds of having hypertension by 14%. Finally, both a one unit increase potassium and individuals who have had more than 12 alcoholic drinks in a year are estimated to have no effect on the odds of having hypertension. Unfortunately, figure \ref{fig:hist1} does not produce a sufficient depiction of the odds ratios of Age, Age^2^, BMI, BMI^2^ and Diabetes and the interaction terms associated with these variables.\ 

$\vspace{1.5cm}$

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=4, fig.cap="The odds ratios for the logistic regression. The purple dots indicate increased odds ratios. The green dots indicate decreased odds ratios.\\label{fig:hist1}",fig.show='hold'}
plot_model(model_log1, vline.color = "black", sort.est = TRUE,colors = "Accent")
```

\pagebreak

The partial dependence plot (figure \ref{fig:hist2}) for the logistic regression model reveals how the average prediction for hypertension continues to increase for a one unit increase in age for both males and females. The average prediction for hypertension continues to increase for a one unit increase in BMI for females, however, it is evident there is a sharp decrease in the prediction of hypertension for males when BMI reaches 40. Potassium appears to have a very steady, however minor decrease in the prediction for hypertension for both genders.\ 

$\vspace{1.5cm}$


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Partial dependence plot on the training data for the logistic regression response for age, bmi and potassium, for males and females separately.\\label{fig:hist2}",fig.show='hold'}
plot(model_profile(ex_log,groups="Gender")) + ggtitle("Response for Logistic Regression") + theme(plot.title = element_text(size = 10))
```


## 3.2. Random Forest{#resultsrf}

Similarly,the partial dependence plot for the random forest model (figure \ref{fig:hist3}) reveals how the average prediction for hypertension continues to increase for a one unit increase in age for both males and females. The average prediction for hypertension here continues to increase for a one unit increase in BMI for both females and males. The increasing trend for BMI begins to level off around a BMI of 50. Potassium appears to have a comparable trend to the logistic regression model, the predictions for hypertension for both genders level off quite rapidly. There is a considerable amount of over-fitting accounted for by the random forest model on the training data.\ 


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=3, fig.cap="Partial dependence plot on the training data for the random forest response for age, bmi and potassium, for males and females separately.\\label{fig:hist3}",fig.show='hold'}
plot(model_profile(ex_rf,groups="Gender")) + ggtitle("Response for Random Forest") + theme(plot.title = element_text(size = 10))
```

$\vspace{1.5cm}$

Figure \ref{fig:rf_var} displays the mean decrease in accuracy and the Gini index totaled over splits involving each predictor. The MeanDecreaseAccuracy contains a measure of the extent to which a variable improves the accuracy of the random forest in predicting the classification. The higher the variable is on this plot the more important the variable is and the more the variable improves the prediction of the model. Age is estimated to be the most important variable in the MeanDecreaseAccuracy plot in the random forest model. If the association with Age and Hypertension is removed, there is eveidently the greatest loss of accuracy in the model. The MeanDecreaseGini plot also exhibits the variable importance. MeanDecreaseGini provides a more nuanced measure of importance, which factors in both the contribution that variable makes to accuracy, and the degree of misclassification. This plot provides evidence that Age, BMI and Potassium, all continuous variables are the most importance variables in this data set. The splits on Age are producing the greatest reduction on the Gini index.

\pagebreak



## 3.3. Neural Network{#resultsnn}

In parallel, the partial dependence plot for the neural network model (figure \ref{fig:hist3}) reveals how the average prediction for hypertension continues to increase for a one unit increase in age for both males and females. The average prediction for hypertension here continues to increase for a one unit increase in BMI for both females and males. Potassium appears to have a comparable trend to the contrasting models, the predictions for hypertension for both genders have a slow decreasing trend. There is a considerable amount of over-fitting accounted for by the random forest model on the training data. The neural network model appear to bit the data quite satisfactorily.\ 

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=4, fig.cap="Variable importance for the random forest model.\\label{fig:rf_var}",fig.show='hold'}
varImpPlot(model_rf, main = "")
```


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=3, fig.cap="Partial dependence plot on the training data for the neural network response for age, bmi and potassium, for males and females separately.\\label{fig:hist4}",fig.show='hold'}
plot(model_profile(ex_nn,groups="Gender")) + ggtitle("Response for Neural Network")+ theme(plot.title = element_text(size = 10))
```

\pagebreak

## 3.4. Measures of Accuracy{#accuracy}

Following investigation of all training accuracy and AUC measures, it was clear the random forest model outperformed all other models (figure \ref{tbl:data1}). The random forest model produced extremely high and satisfactory measures (AUC = 1 and accuracy = 0.994). This model shows great indication of a powerful model in predicting hypertension. Using the training data, the logistic regression and neural network models yielded almost identical measures of accuracy and AUC.\ 


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# look at model_performance and plot model_performance for nice way of getting roc and model summaries.
 
rf_auc <- model_performance(ex_rf)$measures$auc
log_auc <- model_performance(ex_log)$measures$auc
rf_acc <- model_performance(ex_rf)$measures$accuracy
log_acc <- model_performance(ex_log)$measures$accuracy
nn_auc <-model_performance(ex_nn)$measures$auc
nn_acc <- model_performance(ex_nn)$measures$accuracy


rf_auc_test <- model_performance(ex_rf_test)$measures$auc
log_auc_test <- model_performance(ex_log_test)$measures$auc
rf_acc_test <- model_performance(ex_rf_test)$measures$accuracy
log_acc_test <- model_performance(ex_log_test)$measures$accuracy
nn_auc_test <-model_performance(ex_nn_test)$measures$auc
nn_acc_test <- model_performance(ex_nn_test)$measures$accuracy


# pred1 <- predict(model_log1, Test, type="response")
# pred1 <- factor(ifelse(pred1 < .5, 0,1))
# tab1 <- table(Test$Hypertension, pred1)
# log_test_error <-(tab1[2]+tab1[3]) / sum(tab1)  # test error
# predb <- predict(model_rf, Test, type = "class")
# rf_test_error <- mean(predb != Test$Hypertension) #around 27%
# pred2 <- factor(ifelse(predict(model_nn, Test, type ="raw") > 0.5,1,0))
# tab3 <- table(pred2, Test$Hypertension)
# nn_test_error <- (tab3[2]+tab3[3]) / sum(tab3) # around 25%

Model = factor(c("Logistic Regression","Random Forest", "Neural Network"))
Training_Accuracy = c(log_acc,rf_acc,nn_acc)  %>% round(digits = 3)
AUC = c(log_auc,rf_auc,nn_auc)  %>% round(digits = 3)
data1=data.frame(Model,Training_Accuracy,AUC)


kable(data1, "latex", col.names =c("Model","Training Accuracy","AUC"),  booktabs=TRUE, linesep="", caption="The performance of all models evaluated using the training data.\\label{tbl:data1}") %>%
    kable_styling(latex_options=c("HOLD_position"))
```


```{r, echo =FALSE, results='hide'}

# Test Errors
# Logistic Regression
predicted1 <- predict(model_log1, Test, type="response")
predicted1 <- factor(ifelse(predicted1 < .5, 0,1))
target1 <- Test$Hypertension
tab1 <- table(target1, predicted1)
tab1
# Logistic Regression  Error rate
# (tab1[2]+tab1[3]) / sum(tab1) *100 # around 25%
# mean(predicted != Test$Hypertension)*100 # test error



# Random Forest
# f2$confusion

# Test error
predicted2 <- predict(model_rf, Test, type = "class")
target2 <- Test$Hypertension
tab2 <- table(target2, predicted1)
#mean(predicted != Test$Hypertension)*100 #around 27%



# Test Data
table(predict(model_nn, Test,type="class"), Test$Hypertension)
predicted3 <- factor(ifelse(predict(model_nn, Test, type ="raw") > 0.5,1,0))
target3 <- Test$Hypertension
tab3 <- table(target3, predicted3)
tab3
# Neural Network test Error rate
# (tab3[2]+tab3[3]) / sum(tab3) *100 # around 25%

# Logistic 
# Specificity 
specificity_lr <- (tab1[1,1])/ (tab1[1,1]+tab1[1,2])
specificity_lr *100 # 82%
# Type 1 error
type1_error_lr <- 1 - specificity_lr
type1_error_lr
#Sensitivity
sensitivity_lr <- (tab1[2,2]) / (tab1[2,1]+tab1[2,2])
sensitivity_lr *100 # 60%
# Type 1 error
type2_error_lr <- 1 - sensitivity_lr
type2_error_lr
# Precision
precision_lr <- (tab1[2,2])/(tab1[1,2]+tab1[2,2])
precision_lr *100 # 66%
# Accuracy
accuracy_lr <- (tab1[4]+tab1[1])/sum(tab1)
accuracy_lr *100 # 74%


# Random Forest
specificity_rf <- (tab2[1,1])/ (tab2[1,1]+tab2[1,2])
specificity_rf *100 # 
# Type 1 error
type1_error_rf <- 1 - specificity_rf
type1_error_rf
#Sensitivity
sensitivity_rf <- (tab2[2,2]) / (tab2[2,1]+tab2[2,2])
sensitivity_rf *100 
# Type 1 error
type2_error_rf <- 1 - sensitivity_rf
type2_error_rf
# Precision
precision_rf <- (tab2[2,2])/(tab2[1,2]+tab2[2,2])
precision_rf *100 
# Accuracy
accuracy_rf <- (tab2[4]+tab2[1])/sum(tab2)
accuracy_rf *100 # 


# Neural Network Test
# Specificity
specificity_nn  <- (tab3[1,1])/ (tab3[1,1]+tab3[1,2])
specificity_nn  *100 # 79%
# Type 1 error
type1_error_nn  <- 1 - specificity_nn 
type1_error_nn 
#Sensitivity
sensitivity_nn  <- (tab3[2,2]) / (tab3[2,1]+tab1[2,2])
sensitivity_nn  *100 # 67%
# Type 1 error
type2_error_nn  <- 1 - sensitivity_nn 
type2_error_nn 
# Precision
precision_nn  <- (tab3[2,2])/(tab3[1,2]+tab3[2,2])
precision_nn  *100 # 65%
# Accuracy
accuracy_nn  <- (tab3[4]+tab3[1])/sum(tab3)
accuracy_nn  *100 # 74%
```

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.width=4,, fig.cap="Logistic regression confusion matrix\\label{fig:cf1}",fig.show='hold'}
suppressMessages(library(yardstick))


logreg1 <- data.frame(target1,predicted1) 
rf1 <- data.frame(target2,predicted2)
nn1 <- data.frame(target3,predicted3)

# The confusion matrix from a single assessment set (i.e. fold)
cm <- conf_mat(logreg1, target1, predicted1)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.width=4, fig.cap="Random forest confusion matrix.\\label{fig:cf2}",fig.show='hold'}
cm <- conf_mat(rf1, target2, predicted2)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.width=4,fig.cap="Neural Network confusion matrix\\label{fig:cf3}",fig.show='hold'}
cm <- conf_mat(nn1, target3, predicted3)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")

```


The test data was utilised to evaluate the fit of all models and to examine all measures of accuracy. All models performed quite similarly. Table \ref{tbl:data2} presents the model performance with measures of accuracy and AUC values. It is clear that all models produced very similar measures of both accuracy and AUC values. The logistic regression model produced marginally highers measures than the neural network model by a decimal of 0.01 (AUC = `r log_auc_test %>% round(digits = 3)` and accuracy =  `r accuracy_lr %>% round(digits = 3)`). Both the logistic regression and neural network models have outperformed the random forest model. It is clear the random forest model over-fit the training data set prior to evaluating the model fit. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
Model = factor(c("Logistic Regression","Random Forest", "Neural Network"))
# Error_Rate = c(log_test_error,rf_test_error,nn_test_error)  %>% round(digits = 3)
Test_Accuracy = c(log_acc_test,rf_acc_test,nn_acc_test)  %>% round(digits = 3)
AUC = c(log_auc_test,rf_auc_test,nn_auc_test)  %>% round(digits = 3)
Specificity <- c(specificity_lr, specificity_rf, specificity_nn) %>% round(digits = 3)
Sensitivity <- c(sensitivity_lr, sensitivity_rf, sensitivity_nn) %>% round(digits = 3)
Type_1 <- c(type1_error_lr, type1_error_rf, type1_error_nn) %>% round(digits = 3)
Type_2 <- c(type2_error_lr, type2_error_rf, type2_error_nn) %>% round(digits = 3)
Precision <- c(precision_lr,precision_rf, precision_nn) %>% round(digits = 3)
acc <- c(accuracy_lr, accuracy_rf, accuracy_nn)
data2 <-data.frame(Model, Specificity, Type_1, Sensitivity, Type_2, Precision,Test_Accuracy,AUC)

kable(data2, "latex", col.names =c("Model", "Specificity", "Type 1 Error","Sensitivity", 
                                   "Type 2 Error", "Precision", "Accuarcy","AUC"),  
      booktabs=TRUE, linesep="", 
      caption="The performance of all models evaluated using the test data.\\label{tbl:data2}") %>%
    kable_styling(latex_options=c("HOLD_position")) 
```

 Figure \ref{fig:hist5} shows the ROC curve of all models using the test data set. The ROC curve was generated by plotting all model performances and setting geom to "roc". From the plot it is clear how similar all model fits are. The logistic regression line is difficult to inspect as it is masked by the neural network line, although both performances were rather consistent. 

$\vspace{1.5cm}$

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=4, fig.cap="Test ROC curve for all models.\\label{fig:hist5}",fig.show='hold'}
plot(model_performance(ex_rf_test),model_performance(ex_log_test),model_performance(ex_nn_test),
     geom = "roc")+ theme(plot.title = element_text(size = 10), plot.tag = element_text(size = 5)) + geom_line(lwd = 0.5) 
```



</body>


\newpage
# 4. Discussion{#diss}

<body>

This project took a machine learning approach for predicting hypertension using data from NHANES data. Using this data, I focused on assessing three alternative models. The models implemented were logistic regression, random forest and neural network. Each model was examined to determine the most appropriate model in predicting hypertension. Additionally, each variable was investigated to establish potential leading causes of hypertension. The findings are considered in 4.1. Model Selection and 4.2. Variable Importance.

</body>


## 4.1. Model Selection{#modsel}

<body>
The most fitting model was selected based on measures obtained from using the test data to evaluate all model fits. The logistic regression model proved to be the best fitting model in predicting hypertension. This result shows contrast to a previous study (Sakr et al., 2018), that saw the random forest model outperform 5 other machine learning models including a neural network model. Although, the variance between model accuracy and AUC values was minuscule, the logistic regression model had the best performance. This model achieved an AUC of 0.806. The results indicate the logistic regression model was the most accurate model in predicting hypertension, as it produced an accuracy measure of 0.743. 


**With a sensitivity of 40%, and specificity of 87%, the artificial neural network model demonstrates that it might be ineffective for healthcare diagnosis in detecting positive occurrences, but the true negative rate demonstrates that the model is effective at finding non-hypertensive patients. URL:https://www.nature.com/articles/s41598-020-67640-z#Sec20 **
</body>


## 4.2. Variable Importance{#varimport}

<body>
This project utilised three machine learning approaches 

At present there are several variables that have proven to be significant in predicting hypertension. One unit increases in both age and BMI are both significant in predicting hypertension. In the logistic regression, the presence of kidney disease, diabetes (including borderline diabetes), persistent smoking and different racial groups have all been seen to prove significant in increasing the odds of hypertension. 

Age was a significant variable in all models. 
</body>

## 4.3. Study Limitations{#limits}

<body>
A major limitation I encountered during this analysis was the large presence of missing values. As I gathered the data from the NHANES it was expected that missing values would be present as I personally had not conducted the survey. These missing values have potential to reduce statistical power. The loss of this data can also introduce bias in the estimation of the parameters. Another limitation was the process of variable selection. NHANES data has an enormous selection of data available, an initial issue was locating appropriate variables for the analysis. I was predominately efficient in discovering these variables, however despite the size of the available data there were some variables I sought after that I could not locate. Additionally, in relation to the size of the data, there were multiple variables that would relate to the same concept however alter in some regard. This hindered variable selection.

</body>

## 4.4. Future Work{#future}

<body>


</body>


## 4.5. Conclusion{#con}

<body>


Globally, cardiovascular diseases are responsible for one-third of mortality (Global health estimates, 2014) and in Ireland, they account for an almost identical proportion of deaths (Vital Statistics Fourth Quarter and Yearly Summary 2011, 2012). Although there was a dramatic drop in mortality rates since 1998, cardiovascular diseases still remain the leading cause of death in Ireland (Bennett, et al., 2006). A significant statistic has seen hypertension as a leading modifiable risk factor for cardiovascular diseases and has been estimated to be responsible for no less than 45% of ischaemic heart disease mortality and 51% of total stroke mortality globally (Global health estimates, 2014). The impact of hypertension is increasing around the world (Zhao, et al., 2013). It was estimated that there were approximately 972 million adults worldwide diagnosed with hypertension in 2000 alone, in the context of an aging population, the number of people diagnosed with hypertension is anticipated to increase by up to 60% to reach 1.5 billion people by 2025 (Kearney, et al., 2005). The need for model based prediction analysis for hypertension is colossal. Predicting hypertension can help establish leading causing of hypertension and assist with prevention measures to decrease the vast amount of people being diagnosed with hypertension every year. The repercussions of hypertension can be detrimental and the need for prevention is now.


This analysis has further supported the findings of a previous study (López-Martínez et al, 2018), revealing similar results for the logistic regression model. In conclusion, the logistic regression model produced an AUC of 0.806. The logistic regression model revealed the odds of having hypertension is much higher for individuals who are female than for males. The odds of having hypertension were significantly increased for non-Hispanic black individuals compared to the rest of the ethnic groups.


+ World Health Organization, 2014. Global health estimates 2014 summary tables: deaths by cause, age and sex, 2000-2012 Geneva. 
+ Cso.ie. 2012. Vital Statistics Fourth Quarter and Yearly Summary 2011. [online] Available at: <https://www.cso.ie/en/csolatestnews/pressreleases/2012pressreleases/pressreleasevitalstatisticsfourthquarterandyearlysummary2011/> [Accessed 12 July 2021].
+ Bennett, K., Kabir, Z., Unal, B., Shelley, E., Critchley, J., Perry, I., Feely, J. and Capewell, S., 2006. Explaining the recent decrease in coronary heart disease mortality rates in Ireland, 1985–2000.* Journal of Epidemiology & Community Health*, 60(4), pp.322-327.
+ Zhao, Y., Yan, H., Marshall, R.J., Dang, S., Yang, R., Li, Q. and Qin, X., 2013. Trends in population blood pressure and prevalence, awareness, treatment, and control of hypertension among middle-aged and older adults in a rural area of Northwest China from 1982 to 2010. *PloS one*, 8(4), pp.e61779.
+ Kearney, P.M., Whelton, M., Reynolds, K., Muntner, P., Whelton, P.K. and He, J., 2005. Global burden of hypertension: analysis of worldwide data. *The Lancet*, 365(9455), pp.217-223.



</body>

\pagebreak

# References{.unnumbered}

<body>

1. Kulkarni, S., O'Farrell, I., Erasi, M. and Kochar, M.S., 1998. Stress and hypertension. *WMJ: official publication of the State Medical Society of Wisconsin*, 97(11), pp.34-38.
2. Kaplan, N.M., 2010. *Kaplan's clinical hypertension*. Lippincott Williams & Wilkins.
3. Sakr, S., Elshawi, R., Ahmed, A., Qureshi, W., Brawner, C., Keteyian, S., Blaha, M. and Al-Mallah, M., 2018. Using machine learning on cardiorespiratory fitness data for predicting hypertension: The Henry Ford ExercIse Testing (FIT) Project. *PLOS ONE,* 13(4), p.e0195344.
4. Staessen, J., Wang, J., Bianchi, G. and Birkenhäger, W., 2003. Essential hypertension. *The Lancet*, 361(9369), pp.1629-1641.
5. Sleight, P., 1993. Smoking and hypertension. *Clinical and experimental hypertension*, 15(6), pp.1181-1192.
6. Lago, R.M., Singh, P.P. and Nesto, R.W., 2007. Diabetes and hypertension. Nature Clinical Practice Endocrinology & Metabolism, 3(10), pp.667-667.
7. Hamrahian, S.M. and Falkner, B., 2016. *Hypertension in chronic kidney disease. Hypertension: from basic research to clinical practice*, pp.307-325.
8. Aneja, A., El-Atat, F., McFarlane, S.I. and Sowers, J.R., 2004. Hypertension and obesity. *Recent progress in hormone research*, 59(1), pp.169-205.

9. Go, A.S., Mozaffarian, D., Roger, V.L., Benjamin, E.J., Berry, J.D., Borden, W.B., Bravata, D.M., Dai, S., Ford, E.S., Fox, C.S. and Franco, S., 2013. Heart disease and stroke statistics—2013 update: a report from the American Heart Association. *Circulation*, 127(1), pp.e6-e245.
10. López-Martínez, F., Núñez-Valdez, E., Crespo, R. and García-Díaz, V., 2020. An artificial neural network approach for predicting hypertension using NHANES data. *Scientific Reports*, 10(1).
11. Alpaydin, E., 2020. *Introduction to machine learning*. MIT press.
12. Marsland, S., 2015.  *Machine learning: an algorithmic perspective*. CRC press.
13. Kononenko, I., 2001. Machine learning for medical diagnosis: history, state of the art and perspective. *Artificial Intelligence in medicine*, 23(1), pp.89-109.
14. Deo, R.C., 2015. Machine learning in medicine. *Circulation*, 132(20), pp.1920-1930.
15. Obermeyer, Z. and Emanuel, E.J., 2016. Predicting the future—big data, machine learning, and clinical medicine. The New England journal of medicine, 375(13), p.1216.
16. Darcy, A.M., Louie, A.K. and Roberts, L.W., 2016. Machine learning and the profession of medicine. Jama, 315(6), pp.551-552.
17. Elshawi, R., Al-Mallah, M.H. and Sakr, S., 2019. On the interpretability of machine learning-based model for predicting hypertension. BMC medical informatics and decision making, 19(1), pp.1-32.
18. Singal, A.G., Rahimi, R.S., Clark, C., Ma, Y., Cuthbert, J.A., Rockey, D.C. and Amarasingham, R., 2013. An automated model using electronic medical record data identifies patients with cirrhosis at high risk for readmission. *Clinical Gastroenterology and Hepatology*, 11(10), pp.1335-1341.
19. He, D., Mathews, S.C., Kalloo, A.N. and Hutfless, S., 2014. Mining high-dimensional administrative claims data to predict early hospital readmissions. *Journal of the American Medical Informatics Association*, 21(2), pp.272-279.
20. Pederson, J.L., Majumdar, S.R., Forhan, M., Johnson, J.A., McAlister, F.A. and PROACTIVE Investigators, 2016. Current depressive symptoms but not history of depression predict hospital readmission or death after discharge from medical wards: a multisite prospective cohort study. *General hospital psychiatry*, 39, pp.80-85.
21. Futoma, J., Morris, J. and Lucas, J., 2015. A comparison of models for predicting early hospital readmissions. *Journal of biomedical informatics*, 56, pp.229-238.
22. Chang, C.D., Wang, C.C. and Jiang, B.C., 2011. Using data mining techniques for multi-diseases prediction modeling of hypertension and hyperlipidemia by common risk factors. *Expert systems with applications*, 38(5), pp.5507-5513.
23. Abdullah, A.A., Zakaria, Z. and Mohamad, N.F., 2011. Design and development of fuzzy expert system for diagnosis of hypertension. *Second international conference on intelligent systems, modelling and simulation*, pp. 113-117.
24. Farran, B., Channanath, A.M., Behbehani, K. and Thanaraj, T.A., 2013. Predictive models to assess risk of type 2 diabetes, hypertension and comorbidity: machine-learning algorithms and validation using national health data from Kuwait—a cohort study. *BMJ open*, 3(5), pp.e002457.
25. Cdc.gov. 2021. NHANES - About the National Health and Nutrition Examination Survey. [online] Available at: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm> [Accessed 31 May 2021].
26. Christopher J. Endres (2021). nhanesA: NHANES Data Retrieval. R package version 0.6.5.3. https://CRAN.R-project.org/package=nhanesA
27. Hall, G.H. and Round, A.P., 1994. Logistic regression—explanation and use. Journal of the Royal College of Physicians of London, 28(3), p.242.
28. John Fox and Sanford Weisberg (2019). An {R} Companion to Applied Regression, Third Edition. Thousand Oaks CA: Sage. URL:https://socialsciences.mcmaster.ca/jfox/Books/Companion
29. Prasad, A.M., Iverson, L.R. and Liaw, A., 2006. Newer classification and regression tree techniques: bagging and random forests for ecological prediction. *Ecosystems*, 9(2), pp.181-199.
30. Ho, T.K., 1995, August. Random decision forests. *In Proceedings of 3rd international conference on document analysis and recognition*, 1, pp. 278-282.
31. Breiman, L., 2001. Random forests. *Machine learning*, 45(1), pp.5-32.
32. Breiman, L., 2002. Using models to infer mechanisms. *IMS Wald Lecture*, 2, pp.59-71.
33. Liaw, A. and Wiener, M. (2002). Classification and Regression by randomForest. R News 2(3), pp. 18-22.
34. Max Kuhn (2021). caret: Classification and Regression Training. R package version 6.0-88. https://CRAN.R-project.org/package=caret
35. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0.
36. Catherine Hurley, Mark O'Connell and Katarina Domijan (2020). condvis2: Interactive Conditional Visualization for Supervised and Unsupervised Models in Shiny. R package version 0.1.1. https://CRAN.R-project.org/package=condvis2
37. Swets, J.A., 1979. ROC analysis applied to the evaluation of medical imaging techniques. *Investigative radiology*, 14(2), pp.109-121.
38. Metz, C.E., 1986. ROC methodology in radiologic imaging. *Investigative radiology*, 21(9), pp.720-733.
39. Hanley, J.A. and McNeil, B.J., 1982. The meaning and use of the area under a receiver operating characteristic (ROC) curve. *Radiology*, 143(1), pp.29-36.
40. Biecek, P. and Burzykowski, T., 2021. *Explanatory model analysis: explore, explain, and examine predictive models*. CRC Press.
41. Biecek P (2018). “DALEX: Explainers for Complex Predictive Models in R.” _Journal of Machine Learning Research_, 19(84), 1-5. <URL: https://jmlr.org/papers/v19/18-416.html>.
42. Lopez-Martinez, F., Schwarcz, A., Núñez-Valdez, E.R. and Garcia-Diaz, V., 2018. Machine learning classification analysis for a hypertensive population as a function of several risk factors. *Expert Systems with Applications*, 110, pp.206-215.


+ Centers for Disease Control and Prevention. 2021. The Role of Potassium and Sodium in Your Diet. [online] Available at: <https://www.cdc.gov/salt/potassium.htm> [Accessed 13 July 2021].

</body>


\newpage

\renewcommand{\thesection}{\Alph{section}}
\setcounter{section}{1}

# Appendix{-}

<body>



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Boxplot displaying the effect of Age on hypertension.\\label{fig:age}",fig.show='hold'}
#Creating plots to visualise the original data with all observations and levels
load("Data/data_11.RData")
ggplot(data_11) +
  geom_boxplot(aes(x= Age, y = Hypertension)) + labs(x="Age", y = "Hypertension") + coord_flip()
#hypertension seems to be affecting older people in both genders
```


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Boxplot displaying the effect of BMI on hypertension.\\label{fig:bmi}",fig.show='hold'}

ggplot(data_11) +
      geom_boxplot(aes(x = BMI, y = Hypertension))+ labs(x="BMI", y = "Hypertension") + coord_flip()
#in all ethnic groups hypertension seems to be affecting older people
```

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Bar chart displaying the effect of Smoking on hypertension.\\label{fig:smoke1}",fig.show='hold'}
ggplot(data_11) +
      geom_bar(aes(x = Smoking, fill = Hypertension),position = "dodge") + xlab("Smoking") + labs(fill= "Hypertension")
#in both cases where people smoke or do not smoke, the proportion of people that do not have hypertension is higher than those who do have it
```


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2, fig.cap="Bar chart displaying the effect of Kidney disease on hypertension.\\label{fig:kidney1}",fig.show='hold'}
ggplot(data_11) +
      geom_bar(aes(x = KidneyF, fill = Hypertension),position = "dodge") + labs(x="Kidney Disease") + labs(fill="Hypertension")
#it seems as though hypertension is more common for people who do not have diabetes, however for those that do have diabetes, there is a higher proportion of people who have hypertension
```



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Bar chart displaying the effect of Diabetes on hypertension.\\label{fig:diabetes}",fig.show='hold'}
ggplot(data_11) +
      geom_bar(aes(x = Diabetes, fill = Hypertension),position = "dodge") + labs(x="Diabetes") + labs(fill="Hypertension")
#it seems as though hypertension is more common for people who do not have diabetes, however for those that do have diabetes, there is a higher proportion of people who have hypertension
```


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Bar chart displaying the effect of Physical activity on hypertension.\\label{fig:pact}",fig.show='hold'}
ggplot(data_11) +
  geom_bar(aes(x = PhysActivity, fill = Hypertension), position = "dodge") + labs(x="Physical Activity", y = "Hypertension") 
#It is obvious that people that do not exercise weekly have a higher proportion of hypertension cases among men and women.
```



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Bar chart displaying the effect of Alcohol use on hypertension.\\label{fig:alco}",fig.show='hold'}
ggplot(data_11) +
  geom_bar(aes(x = Alcohol, fill = Hypertension), position = "dodge") + labs(x="Alcohol", y = "Hypertension") 
#For both men and women with hypertension, it is clear that those who have had 12 alcoholic drinks in a year do have a higher proportion of hypertension cases.
```



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Bar chart displaying the effect of Salt use on hypertension.\\label{fig:salt}",fig.show='hold'}

ggplot(data_11) +
      geom_bar(aes(x = Salt, fill = Hypertension),position = "dodge") + labs(x="Salt",fill="Hypertension")
# it appears that the number of hypertension cases rise as the level of salt added to a meal increases. There is a clear difference for females with hypertension, that there are more cases for when salt used very often compared to never.
```



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Boxplot displaying the effect of Potassium on hypertension.\\label{fig:pot}",fig.show='hold'}
ggplot(data_11) +
      geom_boxplot(aes(x = Potassium, y = Hypertension),position = "dodge") + labs(x="Potassium", y ="Hypertension") + coord_flip()
```


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2, fig.cap="Boxplot displaying the effect of Race and Age on hypertension.\\label{fig:race}",fig.show='hold'}
ggplot(data_11) +
  geom_boxplot(aes(x= Age, y = Hypertension)) + labs(x="Age", y = "Hypertension") + facet_wrap(~Race) +coord_flip() 
#in all ethnic groups hypertension seems to be affecting older people
```



</body>



A couple more key metrics to keep in mind are sensitivity and specificity. Sensitivity is the percentage of true records that you predicted correctly.
Specificity, on the other hand, is to measure what portion of the actual false records you predicted correctly.
-- https://towardsdatascience.com/navigating-the-random-forest-algorithm-in-r-5ccbc0ef70e

