---
title: |
  ![](logo1.jpeg){width=3in}  
  
  $\vspace{2cm}$
  
  Predicting hypertension with a machine learning classification approach using NHANES data.

author: "" 
date: ""
output: 
  pdf_document
linestretch: 1.5
---

$\vspace{2cm}$

\begin{center}

Niamh Harford

Supervisor: Catherine Hurley

Department of Mathematics and Statistics

National University of Ireland, Maynooth

`r format(Sys.time(), '%B, %Y')`

\end{center}


\pagenumbering{gobble}
\newpage 
\pagenumbering{roman}

\newpage
# Declaration 

I, Niamh Harford declare that this thesis is my own work, except where stated through references or in the Acknowledgments and that it is 13,378 words in length.



$\vspace{6cm}$


Signed:………………………………………………………



Date:…………………………………………………………




\newpage
# Abstract{.unlisted .unnumbered}


This study assessed the performance of three machine learning techniques for the prediction of hypertension using the National Health and Nutrition Examination Survey (NHANES) data from 2011/2012 (training data) and 2015/2016 (test data). The machine learning techniques were logistic regression, random forest and neural network. For each classification model, the association between several characteristics in both hypertensive and non-hypertensive individuals was fit using the training data and evaluated with the test data. These characteristics were age, race, gender, smoking, diabetes, kidney disease, BMI, physical activity, alcohol use, salt use and potassium levels. The results from this analysis indicated that the neural network and logistic regression models displayed superior performance in predicting hypertension. The results displayed a sensitivity of 66%, a specificity of 79%, accuracy of 74% and an area under the curve (AUC) of 0.8 for both models. Additionally, the random forest model demonstrated a sensitivity of 65%, a specificity of 77%, accuracy of 0.73% and an AUC value of 0.79. Logistic regression models provide meaningful interpretations of model parameters, therefore this model was chosen as the preferred model for predicting hypertension. The results from the logistic regression model found that non-Hispanic black individuals and individuals with kidney disease had the highest odds of having hypertension among all explanatory variables, as they were estimated to be two times higher. The results further revealed the remaining participants that fell into the category of non-Hispanic white, other Hispanic or other races, in addition to individuals who smoke excessively, who drink more than 12 alcoholic beverages in a year or who occasionally added salt to their meal all had higher odds of having hypertension. The proposed logistic regression model can be used to make reliable inferences on the leading causes of hypertension and to assist medical researchers in the early prevention, detection and management of hypertension. 








\newpage
# Acknowledgements{.unlisted .unnumbered} 


I would like to express my most sincere gratitude to my supervisor Dr. Catherine Hurley for all the support and whose invaluable expertise guided me prosperously through the course of this project. Catherine's guidance and enthusiasm proved to be extremely motivational and pushed me to successfully complete this project. I would also like thank all of my friends and family who have supported me throughout this project.



\newpage

\tableofcontents

\newpage

# List of figures{.unlisted .unnumbered}
\renewcommand{\listfigurename}{}
\listoffigures
\newpage

# List of tables{.unlisted .unnumbered}
\renewcommand{\listtablename}{} <!-- removes default section name -->
\listoftables



\newpage
\pagenumbering{arabic}
# 1. Introduction{#intro}

Hypertension is a condition where systolic blood pressure is greater than 139 mm Hg or a diastolic blood pressure is greater than 89 mm Hg (Kulkarni et al.,1998). Systolic blood pressure is the highest pressure in the arteries when the heart beats and diastolic blood pressure is the lowest pressure in the arteries when the heart is relaxed between beats (Kaplan, 2010). Hypertension can be a severe condition. It is often described as a common, chronic, age-related condition, which can result in severe cardiovascular and kidney complications including stroke, heart disease and kidney disease (Sakr et al., 2018; Staessen et al., 2003). It mainly arises as a complex quantitative characteristic that is affected by varying relationships of genetic and environmental factors, whereby these characteristics have turned out to be extremely harmful habits that have caused weakened heart function due to increased blood pressure (Staessen et al., 2003). 


Hypertension is a leading modifiable threat for cardiovascular disease and estimated to be responsible for at least 45% of ischaemic heart disease mortality and 51% of total stroke mortality globally (World Health Organisation, 2013). In 1998, the National Health and Nutrition Examination Survey (NHANES) estimated that roughly 50 million adults in America were diagnosed with hypertension (Kulkarni et al., 1998). This figure reveals the enormity of the impact hypertension has had on one country. In addition, the American Heart Association reported hypertension prevalence was estimated to be highest among African Americans, 44% of people surveyed that were diagnosed with hypertension were African American (Go et al., 2013). It also estimated that only 53% of all hypertensive adults have their blood pressure under some control (Go et al., 2013). Furthermore, in 2015 hypertension was considered in the group of risk factors for cardiovascular disease that caused up to 17.7 million deaths around the world (López-Martínez et al, 2020). It is extremely apparent how serious hypertension is to our health and the detrimental effects it can potentially have on all life if blood pressure measures are not monitored and controlled. 



There are many things to consider when determining contributing factors of hypertension. It is important to note that in many cases there may be no one specific cause of hypertension and it can more than often be a result of many factors working in conjunction with each other (Kaplan, 2010). Hypertension prevalence has been seen to rise with age, disregarding both the type of blood pressure measurements and the operative thresholds used for diagnosis (Staessen et al., 2003). Additional factors such as race, smoking, obesity, physical inactivity, high salt intake, extensive alcohol use, low potassium levels and chronic conditions like diabetes and kidney disease can all lead to the diagnosis of hypertension (Kaplan, 2010). Some of these factors may be confounded with age, consequently age could change the effect of those factors on hypertension. For example, there are many habits including physical activity, that as people get older they may refrain from doing. It was reported that the relative risk for developing hypertension in inactive individuals with normal blood pressure at rest was estimated between 35% to 70% higher when compared to individuals who are physically active (Kokkinos, Sheriff and Kheirbek, 2011). Interestingly, several epidemiological studies recommended increased physical activity to lower elevated blood pressure measures or to delay or potentially prevent the development of hypertension (Kokkinos, Sheriff and Kheirbek, 2011). Smoking has been seen to increase the risk of hypertension by up to three times, as a consequence of increased risk in vascular damage due to intense surges in the arterial pressure (Sleight, 1993). This comes as no surprise as smoking often causes a temporary immediate increase in blood pressure, in addition to many extreme effects on the body. Hypertension has had an impact on approximately 70% of patients diagnosed with diabetes and is approximately two times more common in individuals with diabetes than in those without diabetes (Lago, Singh and Nesto, 2007). Additionally, chronic kidney disease is known to be both a common cause of hypertension and also a complication of uncontrolled hypertension (Hamrahian and Falkner, 2016). It is important to consider the interaction between kidney disease and hypertension is complicated and can increase the risk of adverse cardiovascular and cerebrovascular issues (Hamrahian and Falkner, 2016). Nowadays, obesity is a general problem in much of the western world and it is widely known to be linked directly with multiple diseases and conditions including hypertension (Aneja et al., 2004). It is recognised that fat cells in our bodies are not merely an immobile organ for storing energy but they also secrete a host of factors that interact with each other and can bring about elevated blood pressure measures (Aneja et al., 2004). Leptin is a hormone that communicates the nutritional status to the central nervous system and peripheral organs (Prolo, Wong, and Licinio, 1998). In particular, the assumed role of leptin in the causation of hypertension through an activation of the sympathetic nervous system and a direct consequence on the kidneys, resulting in increased salt re-absorption and leading to hypertension (Aneja et al., 2004). This reveals some indication of the effect that kidney complications, possible physical inactivity as a consequence of obesity and high salt levels in the body all have on influencing hypertension. Finally, not consuming enough potassium in your diet and consuming excessive amounts of sodium can raise blood pressure measures, thus giving rise to hypertension (The Role of Potassium and Sodium in Your Diet, 2021). It has been reported that an increase in potassium intake has caused a shift in blood pressure measures to a more desirable level (Ekmekcioglu et al., 2016). 


Machine learning is a contemporary data analysis technique with the distinctive ability to learn and ameliorate its performance without being obviously programmed. Machine learning techniques are usually classified as unsupervised learning and supervised learning. The primary goal of machine learning techniques are to develop models that can be used to perform classification, prediction, estimation or similar tasks (Kourou et al., 2015). Machine learning techniques can be extremely advanced and produce very accurate predictions, although there can be difficulty in the interpretation of the effects explanatory variables have on the response variable. As these learning-based techniques are very powerful in predicting models, there is an obvious growth in the use of these methods for medical researchers. Machine learning techniques can reveal relationships and patterns between variables, from elaborate data sets, while they are capable of effectively predicting future outcomes of medical conditions (Kourou et al., 2015). However, highly trained medical personnel can find it difficult to depend on such models in practice for numerous reasons (Kononenko, 2001). The majority of accessible predictive models often focus on certain diseases and reply upon the knowledge of highly medical researchers (Elshawi, Al-Mallah and Sakr, 2019). Applying predictive models on big health information systems may not perform efficiently as a result of the availability of multiple, elaborate data sources and the diverse mixture of both patients and diagnoses (Elshawi, Al-Mallah and Sakr, 2019). Additionally, the majority of models that are developed by data analysts mainly focus on the prediction accuracy as a performance metric but rarely produce a meaningful explanations of their prediction (Elshawi, Al-Mallah and Sakr, 2019). Despite these arguments there is still a continuous growth in predicting medical conditions and diseases.


Recently, various studies have been using models to predict hypertension in particular (Elshawi, Al-Mallah and Sakr, 2019). The primary goal of these studies was to determine the best fitting model in predicting hypertension. Many papers focused on adopting multiple models using machine learning techniques which have included neural network models, logistic regression models and random forest models to make some inference on hypertension. These papers focused on using training and test data, to assess and evaluate model error rates and accuracy values to determine the most superior machine learning approach to predict hypertension. One analysis focused on fitting a logistic regression model to predict hypertension, (López-Martínez et al., 2018), whereas another analysis solely implemented an artificial neural network model and carried out a thorough examination to compare previous studies that used alternative models (López-Martínez et al., 2020). López-Martínez et al. (2020) revealed their artificial neural network model had an improved overall performance and accuracy in contrast to a previous study they worked on and published using the logistic regression model (López-Martínez et al., 2018). Both of these models used NHANES data in addition to examining the same explanatory variables. These explanatory variables were race, age, smoking, BMI, diabetes, and kidney conditions. The artificial neural network model produced an area under the curve (AUC) value of 0.77, whereas the logistic regression model produced an AUC of 0.73. From the comparison of these two models analysed by López-Martínez et al. (2018, 2020), the paper provided evidence that an artificial neural network was a more advanced model in predicting hypertension. Another study assessed six machine learning techniques to establish the most fitting model to predict hypertension, these models were logitBoost, Bayesian network classifier, locally weighted naive Bayes, artificial neural network, support vector machine and random forest with data from the Henry Ford Health Systems (Sakr et al., 2018). This analysis found that the random forest model outperformed all other models and produced an AUC of 0.93. All three analysis have displayed the strength of alternative machine learning techniques and how successful they can be implemented with predicting medical conditions like hypertension.


Predicting hypertension can help establish leading causes of hypertension. Furthermore, using models to predict hypertension can contribute to preventing extensive occurrences of hypertension cases, by offering useful advise in maintaining healthy blood pressure measures. A model will be built that provides practical and beneficial information regarding hypertension. Three machine learning techniques will be employed to determine which model is the most superior in predicting hypertension. These three techniques are logistic regression, random forest and neural network models. Important contributing factors of hypertension will be determined by exploring the effect of multiple variables on hypertension. These variables will examine the effect of age, gender, race, smoking, diabetes, kidney disease, BMI, physical activity, alcohol use, salt use and potassium levels on hypertension. The selected variables have all been reported to have various effects on the prevalence of hypertension. Data from the NHANES will be used. NHANES is an application of research created to evaluate the physical and nutritional status of both children and adults in America (About the NHANES, 2021). This data is published online and can be publicly accessed for extensive health related statistical analysis. The survey is distinctive in that it uses both interviews and physical assessments to examine the health of the participants who are being sampled. The NHANES was first established in the early 1960s and has ran numerous surveys focusing on different population groups or health topics. In 1999, the survey became a sustained program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs (About the NHANES, 2021). NHANES surveys a nationally representative sample of about 5,000 persons annually, where a complex, multi-step sampling plan is used to choose this sample representative of people in America. The NHANES interview includes many components in particular demographic, socioeconomic, dietary and health-related questions. The physical assessment component consists of medical, dental, and physiological measurements, as well as laboratory tests dispensed by highly trained medical personnel (About the NHANES, 2021). Findings from surveys such as NHANES are used to discover the prevalence of major diseases and risk factors for diseases (About the NHANES, 2021).


\newpage
# 2. Data{#data}




## 2.1. Data Source{#setup}


The data that was collected was from NHANES 2011/2012 and 2015/2016 data sets. The 2011/2012 data was used as the training data to fit the model. The 2015/2015 data was used as the test data to evaluate the fit of the models. The initial stages of data collection involved examining potential explanatory variables and determining their appropriateness to the analysis. Firstly, explanatory variables that previous studies applied to their analysis were examined as these were reported to have substantial effects on hypertension. There were several variables that were originally chosen for the data, however large quantities of missing values were identified within these variables. Consequently, these such variables were disregarded. After thoroughly examining all potential variables, 11 explanatory variables were ultimately chosen to contribute to the analyses of predicting hypertension. The data was imported in R (Version 1.4.1120) using the 'nhanesA' package (Endres, 2021). Eight data files were imported from both the training and test data sets. Six out of the eight files came from the questionnaire data, these files were SMQ_G, DIQ_G, KIQ_G, BPQ_G, PAQ_G and ALQ_G. The remaining two files came from the dietary and examination data, these files were DR1TOT_G and BMX_G. Finally, the demographics table was imported, the file was DEMO_G. The demographic table contains personal information about the participants which includes age and ethnicity. All of these files were merged to form one data set.  The 11 explanatory variables and one response variable were extracted from the data sets to form the final data set. The explanatory variables were RIAGENDR, RIDAGEYR, RIDRETH1, SMQ020, DIQ010, KIQ022, BMXBMI, PAQ665, ALQ101, DRQSPREP and DR1TPOTA and the response variable was BPQ020 (table \ref{tbl:data0}). This process was carried out for both the training and test data sets. Each variable contained a question where the participants response or measurements were recorded. Table \ref{tbl:data0} summarises all variables, what these variables were renamed and the description or question corresponding to each variable. The entire training data contained 5319 participants and the test data contained 5473 participants prior to any data cleaning processes. NHANES data are often coded according to their unique procedure, consequently it was essential to translate the variables using the function nhanesTranslate(). This meant that any categorical variables that were coded with values such as 0 and 1 would be translated to responses participants filled out, for example "No" and "Yes". This process allows for a comprehensive interpretation of all categorical variables.


 
```{r, include=TRUE, echo=FALSE, message=FALSE}
suppressMessages(library(kableExtra))
Data <- factor(rep(c("Questionaire","Examination", "Dietary","Demograpic"), c(6,1,2,3)))
Files <- factor(rep(c("BPQ_G","SMQ_G", "DIQ_G", "KIQ_G", "PAQ_G", "ALQ_G", "BMX_G", 
                      "DR1TOT_G", "DEMO_G"),c(1,1,1,1,1,1,1,2,3)))
Variables <- factor(c("BPQ020", "SMQ020", "DIQ010", "KIQ022", "PAQ665", "ALQ101", "BMXBMI", "DRQSPREP", "DR1TPOTA", "RIAGENDR", "RIDAGEYR", "RIDRETH1"))
Description <- factor(c("Have you ever been told that you have hypertension?",
                      "Have you smoked more that 100 cigarettes during your lifetime?","Have you ever been told that you have diabetes?","Have you ever been told that you have weak or failing kidneys?", "In a normal week would you do any moderate-intensity sports?","In any one year, have you had at least 12 of any type of alcoholic drinks?","Body Mass Index (kg/m^2^)","How often would you be likely to add salt to your food at the table?","Potassium (mg)", "Gender of participants", "Age in years, individuals who are 80 years of age and over are top-coded", "Race or ethnic group of participants"))
Renamed <- factor(c("Hypertension","Smoking","Diabetes","KidneyF","PhysActivity","Alcohol","BMI","Salt","Potassium", "Gender","Age","Race"))

data=data.frame(Variables,Renamed,Description)

kable(data, "latex", booktabs=TRUE, linesep="", caption="NHANES selected variables.\\label{tbl:data0}") %>%
    kable_styling(latex_options=c("HOLD_position"))

```




## 2.2. Data Exploration and Cleaning {#explor}

To explore both the training and test data, data visualisation techniques were used to examine whether any patterns or relationships were present among the variables. Each explanatory variable was plotted with the response variable to assess if such patterns were present. After exploring the plots, several issues were identified. Firstly, it was apparent there were a considerable amount of outliers present in Potassium for both the training and test data (Figure \ref{fig:pot}). Potassium had very few extremely large values, whereas the majority of values within this variable were considerably small, between a range of 0 and 5000. Potassium was log transformed for both training and test data sets to reduce the skewness of the data. 

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=1.8, fig.cap="Boxplot displaying the effect of potassium on hypertension for the training data.\\label{fig:pot}",fig.show='hold'}
suppressMessages(library(naniar))
suppressMessages(library(tidyverse))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(gtable))

load("Data/all_dat.RData")
load("Data/data_11.RData")

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") 

ggplot(data_11) + geom_boxplot(aes(x = Potassium, y = Hypertension)) + labs(x="Potassium", y ="Hypertension") + coord_flip() 

```

\pagebreak

Furthermore, it was clear both data sets were comprised of a considerable number of missing values (Figure \ref{fig:hist0}). The training data had total of 1712 missing values, which were present across 903 rows of the data. The test data had a total of 1453 missing values, which were present across 754 rows of the data. This quantity of missing values was a substantial proportion of both the training data (0.322) and test data (0.265). Several options were explored to deal with missing values. These options included omitting rows of data that contained missing values or imputing the missing values. It was evident three variables in particular stood out as having the large quantities of missing values. These variables were alcohol, potassium and salt. 



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3.7, fig.cap="The number of missing values from the raw data.\\label{fig:hist0}"}
x <- gg_miss_var(data_11) + theme_bw() + labs(title ="2011/2012 Training Data", y = "") #All data 2011/2012

y <- gg_miss_var(all_dat) + theme_bw() + labs(title ="2015/2016 Test Data", y = "Number of missing values") #All data 2015/2016

g2 <- ggplotGrob(x)
g3 <- ggplotGrob(y)
g <- rbind(g2, g3, size = "first")
g$widths <- unit.pmax(g2$widths, g3$widths)

plot(g)
```

    
The levels of all categorical variables were inspected through both analysis and by visually exploring the data. There were two levels present in some of the categorical variables that stood out. These levels were "Don't Know" and "Refused". Participants would have had the opportunity to respond with these responses during the survey. Through further assessment, it was evident there were either no observations or very little observations in these levels. Thus these levels did not provide any useful information for modelling and also made interpretation difficult. The most compelling variables were selected to highlight some of these issues that were encountered. Figure \ref{fig:b4clean1} and \ref{fig:b4clean2} display the "Don't Know" and "Refused" levels present in the variables and also highlights the presence of missing values for the smoking, alcohol and salt variables, prior to any cleaning processes. 

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5, fig.width=9.7, fig.cap="Bar charts displaying the effect of smoking and alcohol use on hypertension for the training data, prior to data cleaning.\\label{fig:b4clean1}",fig.show='hold'}
x <- ggplot(data_11) +
      geom_bar(aes(x = Smoking, fill = Hypertension)) + xlab("Smoking") + labs(fill= "Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)]) 
y <- ggplot(data_11) +
      geom_bar(aes(x = Alcohol, fill = Hypertension)) + xlab("Alcohol") + labs(fill= "Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)]) 


suppressMessages(library(patchwork))

combined1 <- x + y & theme(legend.position="bottom")
combined1 + plot_layout(guides = "collect")
```


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5, fig.width=9.7, fig.cap="Bar charts displaying the effect of salt use and diabetes on hypertension for the training data, prior to data cleaning.\\label{fig:b4clean2}",fig.show='hold'}
z <- ggplot(data_11) +
      geom_bar(aes(x = Salt, fill = Hypertension)) + labs(x="Salt",fill="Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)])

w <- ggplot(data_11) +
      geom_bar(aes(x = Diabetes, fill = Hypertension)) + labs(x="Diabetes") + labs(fill="Hypertension")  + scale_fill_manual(values=cbPalette[c(3,4,5)])

combined2 <- z + w & theme(legend.position="bottom")
combined2 + plot_layout(guides = "collect")
```



After some consideration, the decision was made to remove the observations where participants responded either "Don't Know" or "Refused". Following the removal of these levels the missing values from both the training and test data were omitted. Only 52 missing values from the training data and 45 missing values from the test data were omitted. Following this, all categorical variables were examined to verify they were marked as factors and continuous variables marked as numeric in R. The final cleaning process involved reordering some of the categorical variables using the relevel() function. This reordered the selected categorical response placing "No" first. After completing all data cleaning processes, the training data now had a total of 4312 participants and the test data had a total of 4624 participants. The response variable had a total of 1576 individuals diagnosed with hypertension and 2376 individuals who did not have hypertension for the training data. In the test data, the response variable had a total of 1711 individuals diagnosed with hypertension and 2913 individuals who did not have hypertension. Figures \ref{fig:after} and \ref{fig:after2} display the previously selected variables following all data cleaning processes.


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=9.7, fig.cap="Bar charts displaying the effect of smoking and alcohol use on hypertension for the training data, following data cleaning.\\label{fig:after}",fig.show='hold'}
load("Data/data1_11a.RData")
x <- ggplot(data_11a) +
      geom_bar(aes(x = Smoking, fill = Hypertension)) + xlab("Smoking") + labs(fill= "Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)]) 
y <- ggplot(data_11a) +
      geom_bar(aes(x = Alcohol, fill = Hypertension)) + xlab("Alcohol") + labs(fill= "Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)]) 



combined <- x + y & theme(legend.position="bottom")
combined + plot_layout(guides = "collect")
```



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=9.7, fig.cap="Bar charts displaying the effect of salt use and diabetes on hypertension for the training data, following data cleaning.\\label{fig:after2}",fig.show='hold'}
z <- ggplot(data_11a) +
      geom_bar(aes(x = Salt, fill = Hypertension)) + labs(x="Salt",fill="Hypertension") + scale_fill_manual(values=cbPalette[c(3,4,5)])

w <- ggplot(data_11a) +
      geom_bar(aes(x = Diabetes, fill = Hypertension)) + labs(x="Diabetes") + labs(fill="Hypertension")  + scale_fill_manual(values=cbPalette[c(3,4,5)])

combined <- z + w & theme(legend.position="bottom")
combined + plot_layout(guides = "collect")
```
\pagebreak



\clearpage
# 3. Methods{#methods}


This analysis examined three machine learning classification techniques in predicting hypertension. These techniques were logistic regression, random forest and neural network models. All statistical analysis were performed in R (Version 1.4.1120). The response variable Hypertension is binary taking on either "Yes" or "No" responses, however it was re-coded to take on values of either 1 or 9 as numerical responses are further recognised in all model forms. The 'tidymodels' package (Kuhn et al., 2020) was utilised to execute all three models, as 'tidymodels' are more flexible and often make it easier to specify the models being assessed. The 'tidymodels' framework involves a collection of packages for modelling and machine learning techniques using tidyverse principles. The 'tidymodels' package was an extension of 'caret' and was created to form a uniform interface for the massive variety of machine learning models that exist in R, therefore it can be used to develop multiple types of models with greater consistency compared to other R packages. 



## 3.1. Logistic Regression{#logreg}


Logistic regression models are such models that generally deal with binary dependent variables (Hall and Round, 1994). This model will always produce probabilities between 0 and 1. Odds ratios which are the ratios of the probability of the event occurring to the probability of the event not occurring, are utilised for logistic regression models to provide important associations between predictors and the response  (Hall and Round, 1994). The equation for logistic regression is defined below:

\begin{center}
\[P(Y=1|X=x) = \dfrac{e^{\small \eta}}{1+e^{\small \eta}},\]\ 
\end{center}

where ${\small \eta}$ is the systematic component of the model, e.g. ${\small \eta} = \beta_0 + \beta_1x_1 + . . . +\beta_kx_k$. Several logistic regression models were fitted to the data to determine the most satisfactory model in predicting hypertension. Base R was initially used to determine a good logistic regression model. Binned residual plots were utilised to assess both the overall fit of the logistic regression models for binary outcomes and the inclusion of all three continuous variables. Binned residual plots indicate a good model if approximately 95% of the points lie within the confidence limits. The first model fit explored the relationship between all explanatory variables and the response. To further assess the fit of the two continuous variables Age and BMI, the addition of a quadratic term to both these variables were adjoined to the existing model. The anova() function was used to compare all logistic regression models, using the likelihood ratio test (LRT) to assess the goodness of fit between the two competing models based on the ratio of their likelihoods to establish the best model. The second model including the quadratic terms on Age and BMI, proved to be a better fit model to the data. Exploring the effects between gender and the remaining explanatory variables was a key interest, therefore a model was fit where all variables, inclusive of the quadratic terms had an interaction with Gender. Anova() revealed the quadratic terms Age^2^ and BMI^2^ had the most significant interaction with Gender for the third model. Proceeding with this, a fourth model was fit where Gender had an interaction with Age, Age^2^, BMI and BMI^2^ only. Now anova() was used to determine the best model by computing a LRT for the two models. It was apparent the fourth model where Gender had an interaction with Age, Age^2^, BMI and BMI^2^ was more fitting. The final model fit was an extension to the fourth model as one more interaction term with Age and Diabetes was added. Anova() provided evidence that this additional interaction was significant. Using anova(), the LRT provided evidence that the final model was the most powerful model. The final model chosen for the logistic regression model is defined below:


\begin{center}

log(Hypertension) $= \beta_0 + \beta_1$(Age) $+ \beta_2$(Age$^{2}$) $+ \beta_3$(BMI) $+ \beta_4$(BMI$^{2}$) $+ \beta_5$(Age$   \times$ Gender) $+ \beta_6$(Age$^{2}   \times$ Gender) $+ \beta_7$(BMI $  \times$ Gender) $+ \beta_8$(BMI$^{2}   \times$ Gender) $+ \beta_9$(Race) $ + \beta_{10}$(Smoking) $+ \beta_{11}$(Diabetes) $+ \beta_{12}$(Diabetes $\times$ Age) $+ \beta_{13}$(KidneyF) $+ \beta_{14}$(PhysActivity) $+ \beta_{15}$(Alcohol) $+ \beta_{16}$(Salt) $+ \beta_{17}$(Potassium) 


\end{center}


It is worth noting, through inspection of the binned residual plots all models appeared to fit the data well. The binned residual plot was produced for the final model, as is expected, there were a few points that were outside the confidence limits, however there were no systematic patterns in the plots (Figure \ref{fig:brp}). This model was then fit with 'tidymodels', using the function logistic_reg(). The engine was set to "glm". I examined the odds ratios for this logistic regression model by exploring the exponential of the estimates produced from the tidy() function.


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.cap="Binned residual plot assessing the model fit for the final logistic regression model. \\label{fig:brp}",fig.show='hold'}
suppressMessages(library(arm))
f5 <- glm(Hypertension ~ (Age+I(Age^2) + BMI+I(BMI^2))*Gender+Race + Smoking + (Diabetes*Age) + KidneyF + PhysActivity + Alcohol + Salt + Potassium, data=data_11a, family = binomial())

binnedplot(fitted(f5), 
           residuals(f5, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "", 
           cex.pts = 0.8, 
           col.pts = "black", 
           col.int = "black")
```



## 3.2. Random Forest{#rf}

Random forest models are an amalgamation of tree predictors where each tree relies on the values of a random vector that was sampled independently, with identical distributions for all trees in the forest (Breiman, 2001). Random forest models were created to generate accurate predictions for a model that does not over-fit the data (Breiman, 2001). The quantity of trees utilised in random forest models is controlled by the user, the default number of trees used is 500. A random forest model was fit using 'tidymodels', using the rand_forest() function with the mode set to "classification". The engine was set to "ranger" (Wright and Ziegler, 2017) and importance set to "impurity". The "impurity" measure is the Gini index for classification. Random forest models use bootstrap samples to construct multiple trees, although each tree is created with a randomised subset of predictors (Prasad, Iverson and Liaw, 2006). The model fit assessed the relationship between the response variable and all explanatory variables.


## 3.3 Neural Network{#nn}

A neural network model is called a network as they can be associated with a directed acyclic graph describing how the functions are composed together. Neural networks contain an input layer, one or more hidden layers, and an output layer. More than often these models are more flexible than the logistic regression models (López-Martínez et al, 2020). Each node produced by neural networks is known as a unit. Each unit connects to another and has an associated weight and threshold. A neuron unit first takes a linear combination of the explanatory variables and applies a non-linear function $f$, called an activation function. The neural network model was fit with 'tidymodels', using the mlp() function with the mode set to "classification". The engine was set to "nnet" (Venables and Ripley, 2002). The model fit assessed the relationship between the response variable and all explanatory variables.


It was necessary to further tune the neural network model using 'tidymodels', as its fit was not satisfactory. The model fit was updated specifying tune() for the hidden_units, penalty and epochs. A grid was created using the grid_regular() function and the range for tuning each argument was specified. A range of 1 to 20 was defined for the hidden_units, 0.5 to 0.0000001 for the penalty and 10 to 10000 for the epochs. The final model was chosen based off the best "roc_auc" value. The final neural network model contained 20 hidden units, a penalty of	1.77828 and 5005 epochs.

</body>


## 3.4. Comparing Models{#comparemodels}


Several methods including graphical and numerical comparisons were employed to evaluate the fit of all models with the test data. The following were examined: the confusion matrix, the receiver operating characteristic (ROC) curve, the AUC and partial dependent (PD) profiles and performances (Friedman, 2001). 
 

Confusion matrices were examined to investigate the performance of classification models on the test data by calculating key measures such as sensitivity, specificity, type 1 error rate, type 2 error rate, accuracy and precision. TP refers to true positive where both actual and predicted values are positive. TN refers to true negative where both actual and predicted values are negative. FP refers to false positive where the predicted value is positive however the actual value is negative. FN refers to false negative where the predicted value is negative however the actual value is positive. N refers to the sum of all negative actual values and P refers to the sum of all positive actual values. Sensitivity is the true positive rate (TPR), which is calculated by TP/P. Specificity is the true negative rate (TNR), which is calculated by TN/N. Type 1 error is the false positive rate (FPR), which is calculated by FP/N. Type 2 error is the false negative rate (FNR), which is calculated by FN/P. Accuracy is calculated by (TP + TN)/N. Finally, precision is the positive predictive value, which is calculated by TP/(TP + FP). 


The ROC curve evaluates the accuracy of models. This curve is the plot of sensitivity versus 1-specificity , also known as type 1 error. It reveals the trade-off between the FPR and TPR as one changes the criterion for positivity (Swets, 1979). The AUC is another technique to measure the accuracy of models. The AUC and ROC curve work in conjunction with each other. The ROC curve provides a visual representation of how accurate the model is, whereas the AUC is the measure of the area under the ROC curve and will produce a figure between 0 and 1. An AUC value of 1 is a model that is perfectly accurate. The AUC value produces a summary of the entire location of the ROC curve instead of depending on a specific operating point (Swets, 1979).


The PD plots are produced using the 'DALEX' package (Biecek, 2018). The main objective of PD profiles is to investigate how the expected value of model prediction will behave as a function of a selected explanatory variable (Friedman, 2001). PD plots are beneficial for comparing alternative models visually (Biecek and Burzykowski, 2021). Friedman (2001)’s PD plots focus on the mean relationship between the predicted response and a set of predictors. This mean is derived from the Independent Conditional Expectation (ICE) plots, as they reveal the estimated relationship for each observation (Goldstein, 2015). The value of the PD profile for model $f()$ and explanatory variables $\underbar{x}_i^{j}$ at $z$, where $z$ is fixed over the marginal distribution of $\underbar{x}$ is defined below:

\[\hat{g}^j _{PD} (z) = \dfrac{1}{n}\sum_{i=1}^{n}f(\underbar{x}_i^{j|=z})\]

Explainers were produced for each model using the explain() function from the 'DALEX' package and the explain_tidymodels() function from the 'DALEXtra' package (Maksymiuk, Gosiewska and Biecek, 2020).  Using these explainers, metrics produced from the model performances for the training and test data were examined. Additionally, the explainers were used to visualise the model performance for each variable.




\newpage
# 4. Results{#results}




## 4.1 Logistic Regression{#resultslogreg}

```{r,echo = F, results='hide',message=FALSE, warning=FALSE}
suppressMessages(library(sjPlot))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(gtable))
suppressMessages(library(ggplot2))
suppressMessages(library(randomForest))
suppressMessages(library(e1071))
suppressMessages(library(DALEX))
suppressMessages(library(rms))
suppressMessages(library(nnet))
suppressMessages(library(neuralnet))
suppressMessages(library(devtools))
suppressMessages(library(DALEXtra))
suppressMessages(library(workflows))
suppressMessages(library(rsample))
# suppressMessages(source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'))
load("Data/data1_11a.RData")

data_11a$Hypertension <-factor(ifelse(data_11a$Hypertension == "Yes", 1 ,0))


set.seed(123)

load("Data/glm_fit.RData")
load("Data/rf_fit.RData")
load("Data/final_nn.RData")

hypn <- as.numeric(data_11a$Hypertension)-1
ex_log <- DALEX::explain(glm_fit, data = data_11a, y = hypn, type = "classification", label = "Logistic Regression", N = NULL, variable_type = "categorical")
ex_rf <- DALEX::explain(rf_fit, data = data_11a, y = hypn, type = "classification", label = "Random Forest", N = NULL, variable_type = "categorical")
ex_nn <-  DALEXtra::explain_tidymodels(final_nn, data = data_11a, y = hypn, type = "classification", label = "Neural Network", N = NULL, variable_type = "categorical")
 
# look at model_performance and plot model_performance for nice way of getting roc and model summaries.

tm_rf_auc <- model_performance(ex_rf)$measures$auc
tm_log_auc <- model_performance(ex_log)$measures$auc
tm_nn_auc <- model_performance(ex_nn)$measures$auc
tm_rf_acc <- model_performance(ex_rf)$measures$accuracy
tm_log_acc <- model_performance(ex_log)$measures$accuracy
tm_nn_acc <- model_performance(ex_nn)$measures$accuracy


load("Data/hyp1.RData")
Test <- hyp1
Test$Hypertension <-factor(ifelse(Test$Hypertension == "Yes", 1 ,0))

hypn1 <- as.numeric(Test$Hypertension)-1
ex_log_test <- DALEX::explain(glm_fit, data = Test, y = hypn1, type = "classification", label = "Logistic Regression", N = NULL, variable_type = "categorical")
ex_rf_test <- DALEX::explain(rf_fit, data = Test, y = hypn1, type = "classification", label = "Random Forest", N = NULL, variable_type = "categorical")
ex_nn_test <-  DALEX::explain(final_nn, data = Test, y = hypn1, type = "classification", label = "Neural Network", N = NULL, variable_type = "categorical")



tbl <- tidy(glm_fit, conf.int = TRUE)
Race_hispanic <- tbl$estimate[7] %>% exp() %>% round(digits = 3)
Race_white <- tbl$estimate[8] %>% exp() %>% round(digits = 3)
Race_black <- tbl$estimate[9] %>% exp() %>% round(digits = 3)
Race_other <- tbl$estimate[10] %>% exp() %>% round(digits = 3)
Smoke <- tbl$estimate[11] %>% exp() %>% round(digits = 3)
Kidn <- tbl$estimate[14] %>% exp() %>% round(digits = 3)
pactivity <- tbl$estimate[15] %>% exp() %>% round(digits = 3)
alco <- tbl$estimate[16] %>% exp() %>% round(digits = 3)
salt_rarely <- tbl$estimate[17] %>% exp() %>% round(digits = 3)
salt_occ <- tbl$estimate[18] %>% exp() %>% round(digits = 3)
salt_voften <- tbl$estimate[19] %>% exp() %>% round(digits = 3)
pota <- tbl$estimate[20] %>% exp() %>% round(digits = 3)


Race_hispanic_ci <- c(tbl$conf.low[7], tbl$conf.high[7]) %>% exp() %>% round(digits = 3)
Race_hispanic_ci <- (paste(Race_hispanic_ci[1],"-", Race_hispanic_ci[2]))
Race_white_ci <- c(tbl$conf.low[8], tbl$conf.high[8] ) %>% exp() %>% round(digits = 3)
Race_white_ci  <- (paste(Race_white_ci[1],"-", Race_white_ci [2]))
Race_black_ci <- c(tbl$conf.low[9],tbl$conf.high[9]) %>% exp() %>% round(digits = 3)
Race_black_ci  <- (paste(Race_black_ci[1],"-", Race_black_ci [2]))
Race_other_ci <- c(tbl$conf.low[10],tbl$conf.high[10]) %>% exp() %>% round(digits = 3)
Race_other_ci <- (paste(Race_other_ci[1],"-", Race_other_ci [2]))
Smoke_ci <- c(tbl$conf.low[11],tbl$conf.high[11]) %>% exp() %>% round(digits = 3)
Smoke_ci <- (paste(Smoke_ci[1],"-", Smoke_ci [2]))
Kidn_ci <- c(tbl$conf.low[14],tbl$conf.high[14]) %>% exp() %>% round(digits = 3)
Kidn_ci <- (paste(Kidn_ci[1],"-", Kidn_ci [2]))
pactivity_ci <- c(tbl$conf.low[15],tbl$conf.high[15]) %>% exp() %>% round(digits = 3)
pactivity_ci <- (paste(pactivity_ci[1],"-", pactivity_ci [2]))
alco_ci <- c(tbl$conf.low[16],tbl$conf.high[16]) %>% exp() %>% round(digits = 3)
alco_ci <- (paste(alco_ci[1],"-", alco_ci [2]))
salt_rarely_ci <- c(tbl$conf.low[17],tbl$conf.high[17]) %>% exp() %>% round(digits = 3)
salt_rarely_ci <- (paste(salt_rarely_ci[1],"-", salt_rarely_ci [2]))
salt_occ_ci <- c(tbl$conf.low[18],tbl$conf.high[18]) %>% exp() %>% round(digits = 3)
salt_occ_ci <- (paste(salt_occ_ci[1],"-", salt_occ_ci[2]))
salt_voften_ci <- c(tbl$conf.low[19],tbl$conf.high[19]) %>% exp() %>% round(digits = 3)
salt_voften_ci <- (paste(salt_voften_ci[1],"-", salt_voften_ci[2]))
pota_ci <- c(tbl$conf.low[20],tbl$conf.high[20]) %>% exp() %>% round(digits = 3)
pota_ci <- (paste(pota_ci[1],"-", pota_ci[2]))



tm_rf_auc_test <- model_performance(ex_rf_test)$measures$auc
tm_log_auc_test <- model_performance(ex_log_test)$measures$auc
tm_nn_auc_test <- model_performance(ex_nn_test)$measures$auc
tm_rf_acc_test <- model_performance(ex_rf_test)$measures$accuracy
tm_log_acc_test <- model_performance(ex_log_test)$measures$accuracy
tm_nn_acc_test <- model_performance(ex_nn_test)$measures$accuracy



tidy(glm_fit, exponentiate = TRUE)  %>%
  filter(p.value < 0.05)
# From this Age, Age^2, BMI, BMI^2, Race non-Hispanic Black, Race non-Hispanic White, SmokingYes, DiabetesYes, KidneyYes, BMI:GenderFemale, I(BMI^2):GenderFemale and Age:DiabetesYes all have a significant effect on hypertension.

```


This analysis used a logistic regression model to predict hypertension. The odds ratios produced from this model were examined and revealed some important increases and decreases in the odds of having hypertension across the explanatory variables. Figure \ref{fig:hist1} displays these odds ratios for the explanatory variables, the green dots indicate increased odds ratios and the blue dots indicate decreased odds ratios of hypertension. Unfortunately, figure \ref{fig:hist1} does not produce a sufficient representation of the odds ratios of Age, Age^2^, BMI, BMI^2^ and Diabetes along with the interaction terms associated with these variables.

```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=3.4, fig.width=5,fig.cap="The odds ratios for the logistic regression model.\\label{fig:hist1}",fig.show='hold'}
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
plot_model(glm_fit, vline.color = "black", sort.est = TRUE,colors = cbPalette[c(3,4)], title = "") + labs(x = "Explanatory Variables")
```

The odds ratios for the explanatory variables that did not have an interaction with another variable are summarised in table \ref{tbl:data3}. All categorical levels within the Race variable were estimated to have increased odds of having hypertension relative to the baseline race, Mexican American. The highest odds ratio within Race was for non-Hispanic black individuals. When you move from individuals who are American Mexican to Non-Hispanic black, the odds of having hypertension were estimated to be two times higher. Other Hispanic, non-Hispanic white and other race individuals all had increased odds of having hypertension which range between 27% to 40% times higher relative to individuals who are American Mexican. Individuals who smoked more than 100 cigarettes in their lifetime had a 26% increase in their odds of having hypertension in comparison to individuals who have not smoked more than 100 cigarettes in their life. For individuals who were diagnosed with kidney disease or failing kidneys, the odds of them having hypertension were estimated to be almost three times higher than those who were not diagnosed with such renal complications. For individuals who were physically active, their odds of having hypertension were estimated to decrease by 10%. For individuals who have had more than 12 alcoholic drinks in a year, the odds of having hypertension were estimated to increase by 4%. An unexpected result included the odds ratios for some categorical levels of salt relative to the baseline "Never". Salt used "Rarely" compared to "Never" was estimated to decrease the odds of having hypertension by 14%. Salt used "Occasionally" was seen to increase the odds of having hypertension by 3%, whereas salt used "Very Often" was estimated to decrease the odds of having hypertension by 2%, both levels relative to "Never" using salt. The extensive use of salt on food at the table was not portrayed to have a significant increase on the effect of hypertension as might be expected. Finally, for a one unit increase in potassium levels the odds of having hypertension were estimated to decrease by 6%. 


```{r, echo = FALSE, include=TRUE,message=FALSE}

Var = factor(c("Race - Other Hispanic","Race - Non-Hispanic White", "Race - Non-Hispanic Black", "Race - Other Race","Smoking - Yes", "Kidney Disease - Yes","Physical Activity - Yes", "Alcohol - Yes", "Salt - Rarely", "Salt - Occasionally", "Salt - Very Often", "Potassium"))
Odds_Ratio = c(Race_hispanic,Race_white,Race_black,Race_other,Smoke,Kidn,pactivity,alco,salt_rarely,salt_occ,salt_voften,pota)
CI = c(Race_hispanic_ci,Race_white_ci,Race_black_ci,Race_other_ci,Smoke_ci,Kidn_ci,pactivity_ci,alco_ci,salt_rarely_ci,salt_occ_ci,salt_voften_ci,pota_ci)
data3=data.frame(Var,Odds_Ratio,CI)

kable(data3, "latex", col.names =c("Variables","Odds Ratios","95% CI"), booktabs=TRUE, linesep="", caption="Odds ratios for logistic regression model\\label{tbl:data3}") %>%
    kable_styling(latex_options=c("HOLD_position"))
```




The PD plot (Figure \ref{fig:hist2}) for the logistic regression model represents the average prediction of the three continuous variables for both males and females. This plot revealed how the average prediction of hypertension continues to increase for a one unit increase in age for both males and females. Similarly, the average prediction of hypertension continued to increase for a one unit increase in BMI for females, however, it is evident there was a sharp decrease in the prediction of hypertension for males when BMI reaches 40. This may be a direct consequence of the lack of male participants with BMI values greater than 40. There are just over double the amount of females who have a BMI greater than 40, compared to the number of males with BMI values over 40. Potassium had a very steady, however minor decrease in the prediction of hypertension for both genders.

\pagebreak


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5, fig.width=5,fig.cap="PD plot of the logistic regression model prediction of the variables Age, BMI and Potassium on hypertension, assessing males and females separately.\\label{fig:hist2}",fig.show='hold'}
plot(model_profile(ex_log,groups="Gender")) + labs(title ="", subtitle = "", x = "Continuous Variables",color = NULL) + theme(plot.title = element_text(size = 10))
```


Figure \ref{fig:log_gen_smoke} shows the average prediction of hypertension for both Gender and Smoking in the logistic regression model. This plot exhibited the average prediction of hypertension to be higher for males than for females although to a lesser degree. The average prediction of hypertension was seen to be higher for individuals who smoke excessively in comparison to those that do not. 



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the logistic regression model prediction of the variables Gender and Smoking on hypertension.\\label{fig:log_gen_smoke}",fig.show='hold'}

pdp_log <- model_profile(ex_log, N = NULL, 
                        variables = c("Gender","Smoking"))

pdp_log$color = "#56B4E9"
plot(pdp_log)+ labs(title ="", subtitle = "") 
```


Figure \ref{fig:log_dia_kid} reveals the average prediction of hypertension for both Diabetes and KidneyF in the logistic regression model. The average prediction of hypertension revealed to be considerably higher for both individuals who have been diagnosed with diabetes or are borderline diagnosed with diabetes. Similarly, the average prediction of hypertension was seen to be noticeably higher for individuals who were diagnosed with kidney disease or failing kidneys.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the logistic regression model prediction of the variables Diabetes and KidneyF on hypertension.\\label{fig:log_dia_kid}",fig.show='hold'}
pdp_log <- model_profile(ex_log, N = NULL, 
                        variables = c("Diabetes","KidneyF"))

pdp_log$color = "#56B4E9"
plot(pdp_log) + labs(title ="", subtitle = "") 
```


Figure \ref{fig:log_alco_pact} shows the average prediction of hypertension for Alcohol and PhysActivity in the logistic regression model. The average prediction of hypertension was estimated to be almost identical for individuals who have and have not had more than 12 alcoholic drinks in a year. Comparably, the average prediction of hypertension was estimated to be somewhat similar for individuals who do and do not exercise, although individuals who are not physically active have a marginally higher prediction of hypertension.



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the logistic regression model prediction of the variables Alcohol and PhysActivity on hypertension.\\label{fig:log_alco_pact}",fig.show='hold'}
pdp_log <- model_profile(ex_log, N = NULL, 
                        variables = c("PhysActivity", "Alcohol"))

pdp_log$color = "#56B4E9"

plot(pdp_log) + labs(title ="", subtitle = "") 
```


Figure \ref{fig:log_salt} shows the average prediction of hypertension for Salt in the logistic regression model. The plot illustrates the average prediction of hypertension is very similar across all levels of Salt. Occasionally and never using salt appear to have the same average prediction. There is evidently no obvious difference between the levels of salt used on a meal.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the logistic regression model prediction of the variable Salt on hypertension.\\label{fig:log_salt}",fig.show='hold'}
pdp_log <- model_profile(ex_log, N = NULL, 
                        variables = c("Salt"))

pdp_log$color = "#56B4E9"

plot(pdp_log) + labs(title ="", subtitle = "") 
```


Figure \ref{fig:log_race} shows the average prediction of hypertension for Race in the logistic regression model. The plot illustrates the average prediction of hypertension is prominent for non-Hispanic black individuals. Non-Hispanic black individuals had the highest average prediction of hypertension in contrast to the other ethnic groups. Mexican American individuals was used as the baseline category for the logistic regression model, it was evident Mexican Americans have the lowest average prediction of hypertension under the logistic regression model.


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=7.3,fig.cap="PD plot of the logistic regression model prediction of the variable Race on hypertension.\\label{fig:log_race}",fig.show='hold'}
pdp_log <- model_profile(ex_log, N = NULL, 
                        variables = c("Race"))

pdp_log$color = "#56B4E9" 

plot(pdp_log) + labs(title ="", subtitle = "") 
```


```{r, eval=FALSE,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2.5, fig.width=5, fig.cap="Variable importance on the training data for the logistic regression model.\\label{fig:rf_var_log}",fig.show='hold'}
suppressMessages(library(vip))

glm_fit %>%
  vip(geom = "point", num_features = 20,
      aesthetics = list(color = "#56B4E9",
                        fill = "#56B4E9")) + ggtitle("")
```


## 4.2. Random Forest{#resultsrf}

This analysis used a random forest model to predict hypertension. The partial dependence plot for the random forest model (Figure \ref{fig:rf_gender}) revealed how the average prediction of hypertension continued to increase for a one unit increase in age for both males and females. In contrast to the logistic regression model, the average prediction of hypertension continued to rise for a one unit increase in BMI for both females and males. The increasing trend for BMI begins to level off around a BMI value of 50. Potassium appeared to have a comparable trend to the logistic regression model, the predictions for hypertension for both genders for a one unit increase in potassium levels proved to also have a slight decreasing trend. However, it is important to consider, in all three plots there is a substantial amount of over-fitting accounted for by the random forest model on the training data, as it is evident that this model is following the data very closely.




```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the random forest model prediction of the variables Age, BMI and Potassium on hypertension, assessing males and females separately.\\label{fig:rf_gender}",fig.show='hold'}
plot(model_profile(ex_rf,groups="Gender")) + labs(title ="", subtitle = "", x = "Continuous Variables") + theme(plot.title = element_text(size = 10))
```



Figure \ref{fig:rf_gen_smok} displays the average prediction of hypertension for Gender and Smoking in the random forest model. This plot revealed the average prediction of hypertension was estimated to be approximately similar for both males and females. The average prediction of hypertension was exhibited to be slightly higher for individuals who smoke excessively in comparison to those that do not, similar to the logistic regression model. 



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the random forest model prediction of the variables Gender and Smoking on hypertension.\\label{fig:rf_gen_smok}",fig.show='hold'}
pdp_rf <- model_profile(ex_rf, N = NULL, 
                        variables = c("Gender", "Smoking"))

pdp_rf$color = "#56B4E9"
plot(pdp_rf) + labs(title ="", subtitle = "")  
```



Figure \ref{fig:rf_dia_kid} illustrates the average prediction of hypertension for Diabetes and KidneyF in the random forest model. This plot revealed the average prediction of hypertension to be reasonably higher for individuals who have been diagnosed with diabetes or are borderline diagnosed with diabetes. Similarly, the average prediction of hypertension was seen to be evidently higher for individuals who were diagnosed with kidney disease or failing kidneys. These results are comparable to the logistic regression model.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the random forest model prediction of the variables Diabetes and KidneyF on hypertension.\\label{fig:rf_dia_kid}",fig.show='hold'}
pdp_rf <- model_profile(ex_rf, N = NULL, 
                        variables = c("Diabetes","KidneyF"))

pdp_rf$color = "#56B4E9"

plot(pdp_rf) + labs(title ="", subtitle = "")  
```



Figure \ref{fig:rf_alco_pact} shows the average prediction of hypertension for Alcohol and PhysActivity in the random forest model. The average prediction of hypertension was estimated to be almost identical for individuals who have and have not had more than 12 alcoholic drinks in a year. Additionally, the average prediction of hypertension was seen to be almost the same for individuals who do and do not exercise, however those that are not physically active appear to have a very minor increase in the prediction of hypertension, although the difference does not look to be significant.



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the random forest model prediction of the variables Alcohol and PhysActivity on hypertension.\\label{fig:rf_alco_pact}",fig.show='hold'}

pdp_rf <- model_profile(ex_rf, N = NULL, 
                        variables = c("PhysActivity", "Alcohol"))

pdp_rf$color = "#56B4E9"

plot(pdp_rf) + labs(title ="", subtitle = "") 
```



Figure \ref{fig:rf_salt} shows the average prediction of hypertension for Salt in the random forest model. The plot illustrates the average prediction of hypertension is very similar across four levels of Salt. Never using salt was found to have the highest average prediction of hypertension, albeit the difference is negligible.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the random forest model prediction of the variable Salt on hypertension.\\label{fig:rf_salt}",fig.show='hold'}
pdp_log <- model_profile(ex_rf, N = NULL, 
                        variables = c("Salt"))

pdp_log$color = "#56B4E9"

plot(pdp_log) + labs(title ="", subtitle = "")  
```



Figure \ref{fig:rf_race} reveals the average prediction of hypertension for Race in the random forest model. The average prediction of hypertension illustrated non-Hispanic black individuals have the highest average prediction of hypertension in contrast to the other racial groups. Non-Hispanic white individuals and individuals who belong to other ethnic groups had the same estimated average prediction of hypertension. Mexican American individuals evidently had the lowest average prediction of hypertension.



```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=7.3,fig.cap="PD plot of the random forest model prediction of the variable Race on hypertension.\\label{fig:rf_race}",fig.show='hold'}
pdp_rf <- model_profile(ex_rf, N = NULL, 
                        variables = c("Race"))

pdp_rf$color = "#56B4E9"

plot(pdp_rf) + labs(title ="", subtitle = "")  
```



Figure \ref{fig:rf_var} illustrates the variable importance for the random forest model on the training data. The  higher the variable is on this plot the more important the variable is and the more the variable improves the prediction of the model. The three continuous variables Age, BMI and Potassium are all shown to be the most important variables in this model. Age in particular proved to be the biggest contributor to Hypertension. Diabetes, Race and Salt are shown to have a considerable importance in this model and effect on Hypertension. Smoking, Gender, PhysActivity, KidneyF and Alcohol all have similar importance in this model, however their importance does not indicate to be sufficient in this model to predict hypertension.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2.3, fig.width=5, fig.cap="Variable importance for the random forest model using the training data.\\label{fig:rf_var}",fig.show='hold'}
suppressMessages(library(vip))
rf_fit %>%
  vip(geom = "point", num_features = 11,
      aesthetics = list(color = "#56B4E9",
                        fill = "#56B4E9")) + ggtitle("")


```



## 4.3. Neural Network{#resultsnn}

This analysis used a neural network model to predict hypertension. The partial dependence plot for the neural network model (Figure \ref{fig:hist4}) demonstrated how the average prediction of hypertension continued to rise for a one unit increase in age for both males and females. Similarly to the random forest model, the average prediction of hypertension here continued to rise for a one unit increase in BMI for both females and males. Potassium appeared to have a noticeably larger decreasing trend compared to the contrasting models. The predictions for hypertension for both genders had a rapid decrease in the average prediction of hypertension for a one unit increase in potassium. 



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2.5,  fig.width=5,fig.cap="PD plot of the neural network model prediction of the variables Age, BMI and Potassium on hypertension, assessing males and females separately.\\label{fig:hist4}",fig.show='hold'}
plot(model_profile(ex_nn,groups="Gender")) + labs(title ="", subtitle = "", x = "Continuous Variables") + theme(plot.title = element_text(size = 10))
```



Figure \ref{fig:nn_gen_smo} shows the average prediction of hypertension for Gender and Smoking in the neural network model. The average prediction of hypertension was estimated to be approximately similar for males and females. Interestingly, the neural network model shows slightly contrasting estimates for individuals who do not smoke. The average prediction of hypertension for individuals who do not smoke was considerably higher in comparison to the previous models. These individuals almost have the same average prediction of hypertension as those individuals who do smoke excessively. 

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the neural network model prediction of the variables Gender and Smoking on hypertension.\\label{fig:nn_gen_smo}",fig.show='hold'}
pdp_nn <- model_profile(ex_nn, N = NULL, 
                        variables = c("Gender", "Smoking"))

pdp_nn$color = "#56B4E9"

plot(pdp_nn) + labs(title ="", subtitle = "") + theme(axis.text.x = element_text(angle = -30)) 
```


Figure \ref{fig:nn_dia_kid} displays the average prediction of hypertension for Diabetes and KidneyF in the neural network model. Further to the previous models, the neural network model estimated similar average predictions of hypertension for both Diabetes and KidneyF. The average prediction of hypertension was estimated to be somewhat higher for individuals who have been diagnosed with diabetes or are borderline diagnosed with diabetes. Similarly, the average prediction of hypertension was higher for individuals who were diagnosed with kidney disease or failing kidneys.




```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the neural network model prediction of the variables Diabetes and KidneyF on hypertension.\\label{fig:nn_dia_kid}",fig.show='hold'}
pdp_nn <- model_profile(ex_nn, N = NULL, 
                        variables = c("Diabetes","KidneyF"))

pdp_nn$color = "#56B4E9"

plot(pdp_nn) + labs(title ="", subtitle = "") + theme(axis.text.x = element_text(angle = -30)) 
```


Figure \ref{fig:nn_alco_pact} reveals the average prediction of hypertension for Alcohol and PhysActivity in the neural network model. Both Alcohol and PhysActivity performed very similarly to the previous logistic regression and random forest models. The average prediction of hypertension was estimated to be approximately the same for individuals who have and have not had more than 12 alcoholic drinks in a year. Additionally, the average prediction of hypertension was seen to be roughly identical for individuals who are and are not physically active.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the neural network model prediction of the variables Alcohol and PhysActivity on hypertension.\\label{fig:nn_alco_pact}",fig.show='hold'}
pdp_nn <- model_profile(ex_nn, N = NULL, 
                        variables = c("PhysActivity", "Alcohol"))

pdp_nn$color = "#56B4E9"


plot(pdp_nn) + labs(title ="", subtitle = "") + theme(axis.text.x = element_text(angle = -30)) 
```



Figure \ref{fig:nn_salt} shows the average prediction of hypertension for Salt in the neural network model. The plot illustrates the average prediction of hypertension is almost identical for all levels of Salt. There is no major variance among the average predictions of hypertension for Salt.


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=5,fig.cap="PD plot of the neural network model prediction of the variable Salt on hypertension.\\label{fig:nn_salt}",fig.show='hold'}
pdp_log <- model_profile(ex_nn, N = NULL, 
                        variables = c("Salt"))

pdp_log$color = "#56B4E9"

plot(pdp_log) + labs(title ="", subtitle = "") 
```



Figure \ref{fig:nn_race} shows the average prediction of hypertension for Race in the neural network model. The average prediction of hypertension revealed non-Hispanic black individuals have the highest average prediction of hypertension in contrast to the other ethnic groups. However, the average prediction of all ethnic groups appeared to be higher to a degree for the neural network model compared to the previous models. In particular, the ethnic groups disregarding non-Hispanic black individuals, have had moderately higher average predictions of hypertension in this model and it is not apparent there are no major differences between these groups on the prediction for hypertension.

\pagebreak

```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5,fig.width=7.3,fig.cap="PD plot of the neural network model prediction of the variable Race on hypertension.\\label{fig:nn_race}",fig.show='hold'}
pdp_nn <- model_profile(ex_nn, N = NULL, 
                        variables = c("Race"))

pdp_nn$color = "#56B4E9"



plot(pdp_nn) + labs(title ="", subtitle = "")
```


Figure \ref{fig:nn_var} reveals the variable importance for the neural network model on the training data. KidneyFYes is the most important variable portrayed in this model. Diabetes, another chronic disease is also shown to be extremely important in predicting hypertension. Additionally, RaceNon-HispanicBlack has proved to have a very strong importance. These results are similar to the odds ratios achieved by the logistic regression, that provided evidence for individuals with kidney disease and non-Hispanic black individuals having the highest odds of having hypertension. SmokingYes is also found to be an important contributor to hypertension. The remaining explanatory variables did not portray strong importance in this model. 


```{r,include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=2.5, fig.width=5, fig.cap="Variable importance for the neural network model using the training data.\\label{fig:nn_var}",fig.show='hold'}

final_nn %>%
  pull_workflow_fit() %>%
  vip(geom = "point", num_features = 20,
      aesthetics = list(color = "#56B4E9",
                        fill = "#56B4E9")) + ggtitle("")

```




## 4.4. Measures of Accuracy{#accuracy}


```{r, echo=FALSE, results='hide'}
suppressMessages(library(yardstick))
# Predict - Logistic Regression
pred_class <- predict(glm_fit,
                      new_data = Test,
                      type = "class")

pred_proba <- predict(glm_fit,
                      new_data = Test,
                      type = "prob")


hyp_results <- Test %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab1 <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target1 <- hyp_results$Hypertension
predicted1 <-hyp_results$.pred_class


# Predict - Random Forest
pred_class <- predict(rf_fit,
                      new_data = Test,
                      type = "class")

pred_proba <- predict(rf_fit,
                      new_data = Test,
                      type = "prob")


hyp_results <- Test %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab2 <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target2 <- hyp_results$Hypertension
predicted2 <-hyp_results$.pred_class


# Predict - Neural Network
pred_class <- predict(final_nn,
                      new_data = Test,
                      type = "class")

pred_proba <- predict(final_nn,
                      new_data = Test,
                      type = "prob")


hyp_results <- Test %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab3 <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target3 <- hyp_results$Hypertension
predicted3 <-hyp_results$.pred_class

```

```{r echo=FALSE, results='hide'}
#TRAIN DATA


# Predict - Logistic Regression
pred_class <- predict(glm_fit,
                      new_data = data_11a,
                      type = "class")

pred_proba <- predict(glm_fit,
                      new_data = data_11a,
                      type = "prob")


hyp_results <- data_11a %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab1_train <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target1_train <- hyp_results$Hypertension
predicted1_train <-hyp_results$.pred_class


# Predict - Random Forest
pred_class <- predict(rf_fit,
                      new_data = data_11a,
                      type = "class")

pred_proba <- predict(rf_fit,
                      new_data = data_11a,
                      type = "prob")


hyp_results <- data_11a %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab2_train <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target2_train <- hyp_results$Hypertension
predicted2_train <-hyp_results$.pred_class


# Predict - Neural Network
pred_class <- predict(final_nn,
                      new_data = data_11a,
                      type = "class")

pred_proba <- predict(final_nn,
                      new_data = data_11a,
                      type = "prob")


hyp_results <- data_11a %>%
  dplyr::select(Hypertension) %>%
  bind_cols(pred_class, pred_proba)



tab3_train <- conf_mat(hyp_results, truth = Hypertension,
         estimate = .pred_class)

target3_train <- hyp_results$Hypertension
predicted3_train <-hyp_results$.pred_class



# Logistic 
# Specificity 
specificity_lr_train <- (tab1_train$table[1,1])/ (tab1_train$table[1,1]+tab1_train$table[1,2])
specificity_lr_train *100 # 78%
# Type 1 error
type1_error_lr_train <- 1 - specificity_lr_train
type1_error_lr_train
#Sensitivity
sensitivity_lr_train <- (tab1_train$table[2,2]) / (tab1_train$table[2,1]+tab1_train$table[2,2])
sensitivity_lr_train *100 # 66%
# Type 1 error
type2_error_lr_train <- 1 - sensitivity_lr_train
type2_error_lr_train
# Precision
precision_lr_train <- (tab1_train$table[2,2])/(tab1_train$table[1,2]+tab1_train$table[2,2])
precision_lr_train *100 # 62%
# Negative predictive value
neg_pred_lr_train <- (tab1_train$table[1,1])/(tab1_train$table[1,1]+tab1_train$table[2,1])
neg_pred_lr_train #81%
# Accuracy
accuracy_lr_train <- (tab1_train$table[4]+tab1_train$table[1])/sum(tab1_train$table)
accuracy_lr_train *100 # 74%


# Random Forest
specificity_rf_train <- (tab2_train$table[1,1])/ (tab2_train$table[1,1]+tab2_train$table[1,2])
specificity_rf_train *100 # 77%
# Type 1 error
type1_error_rf_train <- 1 - specificity_rf_train
type1_error_rf_train
#Sensitivity
sensitivity_rf_train <- (tab2_train$table[2,2]) / (tab2_train$table[2,1]+tab2_train$table[2,2])
sensitivity_rf_train *100 # 64%
# Type 1 error
type2_error_rf_train <- 1 - sensitivity_rf_train
type2_error_rf_train
# Precision
precision_rf_train <- (tab2_train$table[2,2])/(tab2_train$table[1,2]+tab2_train$table[2,2])
precision_rf_train *100 # 60%
# Negative predictive value
neg_pred_rf_train <- (tab2_train$table[1,1])/(tab2_train$table[1,1]+tab2_train$table[2,1])
neg_pred_rf_train #80%
# Accuracy
accuracy_rf_train <- (tab2_train$table[4]+tab2_train$table[1])/sum(tab2_train$table)
accuracy_rf_train *100 # 73%


# Neural Network Test
# Specificity
specificity_nn_train <- (tab3_train$table[1,1])/ (tab3_train$table[1,1]+tab3_train$table[1,2])
specificity_nn_train *100 # 79%
# Type 1 error
type1_error_nn_train <- 1 - specificity_nn_train
type1_error_nn_train
#Sensitivity
sensitivity_nn_train <- (tab3_train$table[2,2]) / (tab3_train$table[2,1]+tab3_train$table[2,2])
sensitivity_nn_train *100 # 66%
# Type 1 error
type2_error_nn_train <- 1 - sensitivity_nn_train
type2_error_nn_train
# Precision
precision_nn_train <- (tab3_train$table[2,2])/(tab3_train$table[1,2]+tab3_train$table[2,2])
precision_nn_train *100 # 64%
# Negative predictive value
neg_pred_nn_train <- (tab3_train$table[1,1])/(tab3_train$table[1,1]+tab3_train$table[2,1])
neg_pred_nn_train #80%
# Accuracy
accuracy_nn_train <- (tab3_train$table[4]+tab3_train$table[1])/sum(tab3_train$table)
accuracy_nn_train *100 # 74%
```

```{r, echo=FALSE, results='hide'}

# TEST DATA
# Logistic 
# Specificity 
specificity_lr <- (tab1$table[1,1])/ (tab1$table[1,1]+tab1$table[1,2])
specificity_lr *100 # 78%
# Type 1 error
type1_error_lr <- 1 - specificity_lr
type1_error_lr
#Sensitivity
sensitivity_lr <- (tab1$table[2,2]) / (tab1$table[2,1]+tab1$table[2,2])
sensitivity_lr *100 # 66%
# Type 1 error
type2_error_lr <- 1 - sensitivity_lr
type2_error_lr
# Precision
precision_lr <- (tab1$table[2,2])/(tab1$table[1,2]+tab1$table[2,2])
precision_lr *100 # 62%
# Negative predictive value
neg_pred_lr <- (tab1$table[1,1])/(tab1$table[1,1]+tab1$table[2,1])
neg_pred_lr #81%
# Accuracy
accuracy_lr <- (tab1$table[4]+tab1$table[1])/sum(tab1$table)
accuracy_lr *100 # 74%


# Random Forest
specificity_rf <- (tab2$table[1,1])/ (tab2$table[1,1]+tab2$table[1,2])
specificity_rf *100 # 77%
# Type 1 error
type1_error_rf <- 1 - specificity_rf
type1_error_rf
#Sensitivity
sensitivity_rf <- (tab2$table[2,2]) / (tab2$table[2,1]+tab2$table[2,2])
sensitivity_rf *100 # 64%
# Type 1 error
type2_error_rf <- 1 - sensitivity_rf
type2_error_rf
# Precision
precision_rf <- (tab2$table[2,2])/(tab2$table[1,2]+tab2$table[2,2])
precision_rf *100 # 60%
# Negative predictive value
neg_pred_rf <- (tab2$table[1,1])/(tab2$table[1,1]+tab2$table[2,1])
neg_pred_rf #80%
# Accuracy
accuracy_rf <- (tab2$table[4]+tab2$table[1])/sum(tab2$table)
accuracy_rf *100 # 73%


# Neural Network Test
# Specificity
specificity_nn <- (tab3$table[1,1])/ (tab3$table[1,1]+tab3$table[1,2])
specificity_nn *100 # 79%
# Type 1 error
type1_error_nn <- 1 - specificity_nn
type1_error_nn
#Sensitivity
sensitivity_nn <- (tab3$table[2,2]) / (tab3$table[2,1]+tab3$table[2,2])
sensitivity_nn *100 # 66%
# Type 1 error
type2_error_nn <- 1 - sensitivity_nn
type2_error_nn
# Precision
precision_nn <- (tab3$table[2,2])/(tab3$table[1,2]+tab3$table[2,2])
precision_nn *100 # 64%
# Negative predictive value
neg_pred_nn <- (tab3$table[1,1])/(tab3$table[1,1]+tab3$table[2,1])
neg_pred_nn #80%
# Accuracy
accuracy_nn <- (tab3$table[4]+tab3$table[1])/sum(tab3$table)
accuracy_nn *100 # 74%
```


All three models produced very similar average predictions of hypertension for all explanatory variables. With the application of the test data to evaluate the fit of all model, all three confusion matrices were created to examine the summary of predicted and actual results for all models (Figures \ref{fig:cf1}, \ref{fig:cf2}, \ref{fig:cf3}). The confusion matrix yielded from the test data and the logistic regression model (Figure \ref{fig:cf1}), revealed an estimated sensitivity of `r sensitivity_lr %>% round(digits = 2) *100`%, which are values of hypertension correctly positively classified as 1 or "Yes" and an estimated specificity of `r specificity_lr %>% round(digits = 2) *100`%, which are values of hypertension correctly negatively classified as 0 or "No". The logistic regression model produced an estimated positive predicted value of `r precision_lr %>% round(digits = 2) *100`%, and a negative predictive value of `r neg_pred_lr %>% round(digits = 2) *100`%. The model produced an estimated false negative rate of `r type2_error_lr %>% round(digits = 2) *100`%, and a false positive rate of `r type1_error_lr %>% round(digits = 2) *100`%.


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=1.5, fig.width=3, fig.cap="Logistic regression confusion matrix using the test data.\\label{fig:cf1}",fig.show='hold'}
suppressMessages(library(yardstick))


logreg1 <- data.frame(target1,predicted1) 
rf1 <- data.frame(target2,predicted2)
nn1 <- data.frame(target3,predicted3)

# The confusion matrix from a single assessment set (i.e. fold)
cm <- conf_mat(logreg1, target1, predicted1)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```



The confusion matrix returned from the test data and the random forest model (Figure \ref{fig:cf2}), provided an estimated sensitivity of `r sensitivity_rf %>% round(digits = 2) *100`%, which are values of hypertension correctly positively classified as 1 or "Yes" and displayed an estimated specificity of `r specificity_rf %>% round(digits = 2) *100`%, which are values of hypertension correctly negatively classified as 0 or "No". The random forest model produced an estimated positive predicted value of `r precision_rf %>% round(digits = 2) *100`%, and a negative predictive value of `r neg_pred_rf %>% round(digits = 2) *100`%. The model produced an estimated false negative rate of `r type2_error_rf %>% round(digits = 2) *100`%, and a false positive rate of `r type1_error_rf %>% round(digits = 2) *100`%.


```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=1.5, fig.width=3, fig.cap="Random forest confusion matrix using the test data.\\label{fig:cf2}",fig.show='hold'}
cm <- conf_mat(rf1, target2, predicted2)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")
```



The confusion matrix produced from the test data and the neural network model (Figure \ref{fig:cf3}). The neural network model reveals an estimated sensitivity of `r sensitivity_nn %>% round(digits = 2) *100`%, and an estimated specificity of `r specificity_nn %>% round(digits = 2) *100`%. The neural network model produced an estimated positive predicted value of `r precision_nn %>% round(digits = 2) *100`%, and a negative predictive value of `r neg_pred_nn %>% round(digits = 2) *100`%. The model produced an estimated false negative rate of `r type2_error_nn %>% round(digits = 2) *100`%, and a false positive rate of `r type1_error_nn %>% round(digits = 2) *100`%.



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=1.5, fig.width=3,fig.cap="Neural network confusion matrix using the test data.\\label{fig:cf3}",fig.show='hold'}
cm <- conf_mat(nn1, target3, predicted3)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")

```


Table \ref{tbl:data2_tr} presents the model performances with use of the training data. It is immediately obvious the random forest model outperformed the other models by examining the metrics produced by the training data. The random forest model produced an almost perfect AUC value at 0.99, in addition to all other measures being extremely high indicating towards a very accurate model. The logistic regression and neural network models in particular performed identically, by producing the same result for all measures assessed.


```{r, echo=FALSE, message=FALSE, warning=FALSE}
Model = factor(c("Logistic Regression","Random Forest", "Neural Network"))
# Error_Rate = c(log_test_error,rf_test_error,nn_test_error)  %>% round(digits = 3)
Test_Accuracy = c(tm_log_acc, tm_rf_acc,tm_nn_acc)  %>% round(digits = 2)
AUC = c(tm_log_auc, tm_rf_auc,tm_nn_auc)  %>% round(digits = 2)
Specificity <- c(specificity_lr_train, specificity_rf_train, specificity_nn_train) %>% round(digits = 2)
Sensitivity <- c(sensitivity_lr_train, sensitivity_rf_train, sensitivity_nn_train) %>% round(digits = 2)
Type_1 <- c(type1_error_lr_train, type1_error_rf_train, type1_error_nn_train) %>% round(digits = 2)
Type_2 <- c(type2_error_lr_train, type2_error_rf_train, type2_error_nn_train) %>% round(digits = 2)
Precision <- c(precision_lr_train,precision_rf_train, precision_nn_train) %>% round(digits = 2)
acc <- c(accuracy_lr_train, accuracy_rf_train, accuracy_nn_train)
data2 <-data.frame(Model, Specificity, Sensitivity, Precision,Test_Accuracy,AUC)

kable(data2, "latex", col.names =c("Model", "Specificity","Sensitivity", 
                                   "Precision", "Accuracy","AUC"),  
      booktabs=TRUE, linesep="", 
      caption="The performance of all models evaluated using the training data.\\label{tbl:data2_tr}") %>%
    kable_styling(latex_options=c("HOLD_position")) 
```



From further examination of the measures produced from the evaluation of the test data, it was evident that all models performed quite similarly. Table \ref{tbl:data2} presents the model performances with measures including accuracy and AUC values. The logistic regression and neural network models in particular produced very similar measures of both accuracy and AUC values. For three out of five of the measures displayed in table \ref{tbl:data2}, the neural network model and the logistic regression model performed identically, as they both produced the same value for the TPR, TNR and accuracy. The logistic regression model produced an AUC value 0.01 higher the AUC value produced by the neural network model, however this difference is marginal. The neural network model yielded the highest precision value. 



```{r, echo=FALSE, message=FALSE, warning=FALSE}
Model = factor(c("Logistic Regression","Random Forest", "Neural Network"))
# Error_Rate = c(log_test_error,rf_test_error,nn_test_error)  %>% round(digits = 3)
Test_Accuracy = c(tm_log_acc_test, tm_rf_acc_test,tm_nn_acc_test)  %>% round(digits = 2)
AUC = c(tm_log_auc_test, tm_rf_auc_test,tm_nn_auc_test)  %>% round(digits = 2)
Specificity <- c(specificity_lr, specificity_rf, specificity_nn) %>% round(digits = 2)
Sensitivity <- c(sensitivity_lr, sensitivity_rf, sensitivity_nn) %>% round(digits = 2)
Type_1 <- c(type1_error_lr, type1_error_rf, type1_error_nn) %>% round(digits = 2)
Type_2 <- c(type2_error_lr, type2_error_rf, type2_error_nn) %>% round(digits = 2)
Precision <- c(precision_lr,precision_rf, precision_nn) %>% round(digits = 2)
acc <- c(accuracy_lr, accuracy_rf, accuracy_nn)
data2 <-data.frame(Model, Specificity, Sensitivity, Precision,Test_Accuracy,AUC)

kable(data2, "latex", col.names =c("Model", "Specificity","Sensitivity", 
                                   "Precision", "Accuracy","AUC"),  
      booktabs=TRUE, linesep="", 
      caption="The performance of all models evaluated using the test data.\\label{tbl:data2}") %>%
    kable_styling(latex_options=c("HOLD_position")) 
```




Figure \ref{fig:roc} shows the ROC curve of all models using the test data set. The ROC curve was generated by plotting all model performances using the 'DALEX' package and setting geom to "roc". This ROC curve displays the similarities between all three models. It is extremely clear there is very little variance between these models when predicting hypertension. 



```{r, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE,fig.height=4, fig.align='center',fig.keep='high',fig.cap="ROC curve for all models using the test data.\\label{fig:roc}",fig.show='hold'}
plot(model_performance(ex_rf_test),model_performance(ex_log_test),model_performance(ex_nn_test),
     geom = "roc") + theme(plot.title = element_text(size = 10), plot.tag = element_text(size = 5)) + geom_line(lwd = 0.5) 
```
\pagebreak



\clearpage
# 5. Discussion{#diss}

To date there have been several studies that have taken various approaches to predict hypertension, by means of utilising alternative models with the use of diverse data sources. This project took a machine learning approach for predicting hypertension using data from the NHANES. The analysis focused on assessing three alternative models, these were logistic regression, random forest and neural network. All models were examined to determine the most advanced model in predicting hypertension. Additionally, each variable was investigated to determine the most important variables when predicting hypertension. The findings are considered below.


The most superior model was selected based on measures obtained through evaluating all models using the test data. These measures included accuracy, precision, specificity, sensitivity, type 1 error rate, type 2 error rate and the AUC value. The logistic regression model achieved an AUC of `r tm_log_auc_test %>% round(digits = 2)` and an accuracy of `r tm_log_acc_test %>% round(digits = 2) *100`%. The neural network model achieved an AUC of `r tm_nn_auc_test %>% round(digits = 2)` and an accuracy of `r tm_nn_acc_test %>% round(digits = 2) *100`%. The random forest model achieved an AUC of `r tm_rf_auc_test %>% round(digits = 2)` and an accuracy of `r tm_rf_acc_test %>% round(digits = 2) *100`%. With a sensitivity of `r sensitivity_lr %>% round(digits = 2) *100`% and specificity of `r specificity_lr %>% round(digits = 2) *100`%, the logistic regression and neural network models illustrated that they can be reasonably effective in detecting positive occurrences of conditions such as hypertension, additionally the TNR demonstrated that both models are more effective at classifying non-hypertensive patients. It is worth noting the neural network model showed a greater precision in predicting hypertension (`r precision_nn %>% round(digits=2) *100`%), than the logistic regression model (`r precision_lr %>% round(digits=2) *100`%). However, the compilation of results indicate both the logistic regression model and neural network model are powerful models in predicting hypertension with the selected explanatory variables from the NHANES. This comes as no surprise as currently, both the logistic regression and neural networks models are the most widely used models in biomedicine (Dreiseitl and Ohno-Machado, 2002). Logistic regression models provide more meaningful interpretations of model parameters, therefore for this reason the logistic regression model was chosen to be the most applicable and accurate model for predicting hypertension and for the determination of important contributing factors of hypertension. It is essential that through machine learning techniques, these models can provide a meaningful and practical manner for medical researchers in the future. The results from this analysis show contrast to a previous study (Sakr et al., 2018), that saw the random forest model outperform 5 other machine learning models including a neural network model. This thesis demonstrated that the random forest model did not perform as successfully as the previous papers. However, this analysis has shown that the random forest model did perform to a satisfactory degree as its performance measures were just below those of the logistic regression and neural network models. 

The logistic regression confirmed that non-Hispanic black individuals produced the highest estimated odds of having hypertension among all ethnic groups. The results build on the findings reported by the American Heart Association that saw the estimates for hypertension prevalence was highest among African Americans (Go et al., 2013). Additionally, all other racial factors explored in this analysis, were estimated to produce higher odds of having hypertension for these individuals in comparison to Mexican Americans. These findings should be taken into consideration when examining and predicting hypertension in the future. Although race is not something that can be changed, it can be utilised in the identification and diagnosis of hypertension henceforward. The logistic regression model estimated increased odds of having hypertension for individuals who smoked over 100 cigarettes in their life and for individuals with kidney disease. The results fit the predicted theory that these such variables would contribute to the cause of hypertension. Excessive smoking has been reported to increase the likelihood of hypertension (Kayame and Mallongi, 2018). Furthermore, a relationship between kidney disease and hypertension was identified, as kidney disease is known to be both a common cause of hypertension and also a complication of unmanaged hypertension (Hamrahian and Falkner, 2016). Kidney disease had the highest, most influential effect on hypertension out of all the variables examined. This result provides a clearer understanding of the relationship between hypertension and kidney disease and the harmful impacts weakened kidneys can have on blood pressure. Additionally, individuals who drank at least 12 alcoholic drinks in a year or who occasionally added salt to their meal also had an estimated increase in the odds of having hypertension, albeit these increases were marginal. The logistic regression model confirmed that individuals who rarely used salt produced the lowest estimated odds of having hypertension out of all variables. Those who used salt very often had a very minor decrease in their estimated odds of having hypertension. The odds ratios produced for salt did not fit the theory. It was expected as the levels of salt use increased the odds ratios would increase too, thus having a growing influence on hypertension. Although, rarely using salt did produce anticipated results, the extensive use of salt did not produce the expected effect on the odds of having hypertension. The number of participants with the salt variable was relatively unbalanced. There were very few participants who never or rarely used salt at the table which may have influenced the results. Finally, this model estimated for a one unit increase in the log of potassium, the odds of having hypertension were decreased. These results demonstrate that as potassium levels increase, the risk of hypertension decreases which builds on previous research that explored increased levels of potassium in the prevention and treatment of hypertension, as potassium has blood pressuring lowering effects (Suter, 1998).


In contrast to the results obtained from the logistic regression model, a previous study (López-Martínez et al, 2018) reported diabetes, kidney disease and smoking habits did not increase the odds of hypertension. The differentiation between the results achieved in this study and those in the previous study (López-Martínez et al, 2018) may be a consequence of the size of the data. López-Martínez et al. (2018) sourced NHANES data from 2007 to 2016, thus their data set would have included and examined thousands of more individuals. This analysis primarily focused on analysing individuals from one year, and evaluated the model fits by employing another year as the test data. Therefore the evaluation process was unique for each analysis carried out. 




## 5.1. Study Limitations{#limits}


Throughout this thesis there were several elements that restricted the progression of this analysis. A major limitation encountered during this analysis was the large presence of missing values. As the data was sourced from the NHANES, it was anticipated that missing values would be present particularly due to the personal nature of some questions asked in this survey. These missing values have potential to reduce statistical power, however the loss of this data can also introduce bias in the estimation of the parameters. Thus the presence and quantity of missing values brought about some complications in the analysis. Another limitation was the process of variable selection. NHANES data has an enormous selection of available variables. An initial issue was sourcing appropriate variables for the analysis. Despite the size of the available data there were some variables I sought after that the NHANES data did not possess. Such variables would have included details regarding family history which could have provided for stronger predictions on hypertension. Imbalanced data was also an issue for this analysis. Several levels within some categorical variables did contain relatively fewer participants in comparison to the other levels in that variable, which may have resulted in imprecise results and producing lower sensitivity values.



## 5.2. Future Work{#future}


Although in this analysis the neural network model produced the same performance measures as the logistic regression model, the neural network more is more advanced with greater techniques. For future work, this analysis would primarily focus on neural network modelling and explore deeper and more progressive techniques to improve predictions of hypertension. The consideration of additional explanatory variables for future work would have the potential to make great advances in predicting hypertension, as well as further contribution to the prevention of hypertension. Ensuring the data is properly balanced will also be a key priority for future work. Although, another approach that could be taken if the data is imbalanced, would be to add weights to the data using the 'survey' package in R. Finally, conducting and exploring national health surveys similar to NHANES in Ireland would be extremely beneficial to citizens in prospect of determining major contributing factors of hypertension here in Ireland. A survey such as this would be so advantageous for data scientists to explore and potentially establish causes of multiple disorders and diseases in Ireland. 


## 5.3. Conclusion{#con}

Globally, cardiovascular diseases are responsible for one-third of mortality (Global health estimates, 2014) and in Ireland, they account for an almost identical proportion of deaths (Vital Statistics Fourth Quarter and Yearly Summary 2011, 2012). Although there was a dramatic drop in mortality rates since 1998, cardiovascular diseases still remain the leading cause of death in Ireland (Bennett, et al., 2006). Previously discussed, hypertension was reported as a major threat for cardiovascular diseases and has been estimated to be responsible for up to 45% of ischaemic heart disease mortality and 51% of total stroke mortality globally (World Health Organisation, 2013). The impact of hypertension is increasing around the world (Zhao, et al., 2013). It was estimated that there were approximately 972 million adults worldwide diagnosed with hypertension in 2000 alone, in the context of an aging population, the number of people diagnosed with hypertension is anticipated to increase by up to 60% to reach 1.5 billion people by 2025 (Kearney, et al., 2005). The need for model based prediction analysis for hypertension is colossal. Predicting hypertension can help establish leading causes of hypertension and assist with prevention measures to decrease the vast amount of people being diagnosed with hypertension every year. The repercussions of hypertension can be detrimental and the need for prevention is now.

By analysing the prediction of hypertension with three alternative machine learning models utilising NHANES data, this thesis has shown that both neural network and logistic regression models are the most accurate at predicting hypertension. The logistic regression model is the most practical model for medical researchers to make meaningful inferences on hypertension for the future prevention and control of this condition. This study has revealed some contrast to previous studies (López-Martínez et al, 2020) that explored several machine learning models that included the random forest and neural network model. However, this analysis has further supported that there are multiple reliable and adequate models that can assist in the prediction of conditions and diseases. This type of analysis can be extremely beneficial to humans all around the world. It has the capability to prevent some of these conditions from transpiring and further preventing extensive cardiovascular deaths globally. 




\pagebreak

# References{.unnumbered}


1. Kulkarni, S., O'Farrell, I., Erasi, M. and Kochar, M.S., 1998. Stress and hypertension. *WMJ: official publication of the State Medical Society of Wisconsin*, 97(11), pp.34-38.
2. Kaplan, N.M., 2010. *Kaplan's clinical hypertension*. Lippincott Williams & Wilkins.
3. Sakr, S., Elshawi, R., Ahmed, A., Qureshi, W., Brawner, C., Keteyian, S., Blaha, M. and Al-Mallah, M., 2018. Using machine learning on cardiorespiratory fitness data for predicting hypertension: The Henry Ford ExercIse Testing (FIT) Project. *PLOS ONE,* 13(4), p.e0195344.
4. Staessen, J., Wang, J., Bianchi, G. and Birkenhäger, W., 2003. Essential hypertension. *The Lancet*, 361(9369), pp.1629-1641.
5. World Health Organization, 2013. A Global Brief on Hypertension: Silent Killer, Global Public Health Crisis. Geneva: World Health Organization.
6. Go, A.S., Mozaffarian, D., Roger, V.L., Benjamin, E.J., Berry, J.D., Borden, W.B., Bravata, D.M., Dai, S., Ford, E.S., Fox, C.S. and Franco, S., 2013. Heart disease and stroke statistics—2013 update: a report from the American Heart Association. *Circulation*, 127(1), pp.e6-e245.
7. López-Martínez, F., Núñez-Valdez, E., Crespo, R. and García-Díaz, V., 2020. An artificial neural network approach for predicting hypertension using NHANES data. *Scientific Reports*, 10(1).
8. Kokkinos, P., Sheriff, H. and Kheirbek, R., 2011. *Physical inactivity and mortality risk. Cardiology research and practice, 2011.*
9. Sleight, P., 1993. Smoking and hypertension. *Clinical and experimental hypertension*, 15(6), pp.1181-1192.
10. Lago, R.M., Singh, P.P. and Nesto, R.W., 2007. Diabetes and hypertension. *Nature Clinical Practice Endocrinology & Metabolism*, 3(10), pp.667-667.
7. Hamrahian, S.M. and Falkner, B., 2016. *Hypertension in chronic kidney disease. Hypertension: from basic research to clinical practice*, pp.307-325.
8. Aneja, A., El-Atat, F., McFarlane, S.I. and Sowers, J.R., 2004. Hypertension and obesity. *Recent progress in hormone research*, 59(1), pp.169-205.
9. Prolo, P., Wong, M.L. and Licinio, J., 1998. Leptin. *The international journal of biochemistry & cell biology*, 30(12), pp.1285-1290.
10. Centers for Disease Control and Prevention. 2021. The Role of Potassium and Sodium in Your Diet. [online] Available at: <https://www.cdc.gov/salt/potassium.htm> [Accessed 13 July 2021].
11. Ekmekcioglu, C., Elmadfa, I., Meyer, A.L. and Moeslinger, T., 2016. The role of dietary potassium in hypertension and diabetes. *Journal of physiology and biochemistry*, 72(1), pp.93-106.
12. Kourou, K., Exarchos, T.P., Exarchos, K.P., Karamouzis, M.V. and Fotiadis, D.I., 2015. Machine learning applications in cancer prognosis and prediction. *Computational and structural biotechnology journal*, 13, pp.8-17.
13. Kononenko, I., 2001. Machine learning for medical diagnosis: history, state of the art and perspective. *Artificial Intelligence in medicine*, 23(1), pp.89-109.
14. Elshawi, R., Al-Mallah, M.H. and Sakr, S., 2019. On the interpretability of machine learning-based model for predicting hypertension. *BMC medical informatics and decision making*, 19(1), pp.1-32.
15. López-Martínez, F., Schwarcz, A., Núñez-Valdez, E.R. and Garcia-Diaz, V., 2018. Machine learning classification analysis for a hypertensive population as a function of several risk factors. *Expert Systems with Applications*, 110, pp.206-215.
16. cdc.gov. 2021. About the NHANES. [online] Available at: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm> [Accessed 31 May 2021].
17. Christopher J. Endres (2021). nhanesA: NHANES Data Retrieval. R package version 0.6.5.3. https://CRAN.R-project.org/package=nhanesA
18. Kuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org
19. Hall, G.H. and Round, A.P., 1994. Logistic regression—explanation and use. Journal of the Royal College of Physicians of London, 28(3), p.242.
20. Breiman, L., 2001. Random forests. *Machine learning*, 45(1), pp.5-32.
21. Wright, M.N. and Ziegler, A., 2017. ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1-17. doi:10.18637/jss.v077.i01
22. Prasad, A.M., Iverson, L.R. and Liaw, A., 2006. Newer classification and regression tree techniques: bagging and random forests for ecological prediction. *Ecosystems*, 9(2), pp.181-199.
23. Venables, W. N. & Ripley, B. D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0
24. Friedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. *Annals of statistics*, pp.1189-1232.
25. Swets, J.A., 1979. ROC analysis applied to the evaluation of medical imaging techniques. *Investigative radiology*, 14(2), pp.109-121.
26. Biecek P (2018). “DALEX: Explainers for Complex Predictive Models in R.” _Journal of Machine Learning Research_, 19(84), 1-5. <URL: https://jmlr.org/papers/v19/18-416.html>.
27. Biecek, P. and Burzykowski, T., 2021. *Explanatory model analysis: explore, explain, and examine predictive models*. CRC Press.
28. Goldstein, A., Kapelner, A., Bleich, J. and Pitkin, E., 2015. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. *Journal of Computational and Graphical Statistics*, 24(1), pp.44-65.
29. Maksymiuk S, Gosiewska A, Biecek P (2020). “Landscape of R packages for eXplainable Artificial Intelligence.” _arXiv_. Pages 6, 7, 11, 15, <URL: https://arxiv.org/abs/2009.13248>.
30. Dreiseitl, S. and Ohno-Machado, L., 2002. Logistic regression and artificial neural network classification models: a methodology review. *Journal of biomedical informatics*, 35(5-6), pp.352-359.
31. Kayame, R. and Mallongi, A., 2018. Relationships between Smoking Habits and the Hypertension Occurrence among the Adults of Communities in Paniai Regency, Papua Indonesia. *Indian Journal of Public Health Research & Development*, 9(1).
32. Suter, P.M., 1998. Potassium and hypertension. *Nutrition reviews*, 56(5), pp.151-153.
33. Global Health Estimates, 2014. *Summary Tables Deaths by Cause, Age and Sex*, 2000–2012. World Health Organization.
34. Cso.ie. 2012. Vital Statistics Fourth Quarter and Yearly Summary 2011. [online] Available at: <https://www.cso.ie/en/csolatestnews/pressreleases/2012pressreleases/pressreleasevitalstatisticsfourthquarterandyearlysummary2011/> [Accessed 12 July 2021].
35. Bennett, K., Kabir, Z., Unal, B., Shelley, E., Critchley, J., Perry, I., Feely, J. and Capewell, S., 2006. Explaining the recent decrease in coronary heart disease mortality rates in Ireland, 1985–2000.* Journal of Epidemiology & Community Health*, 60(4), pp.322-327.
36. Zhao, Y., Yan, H., Marshall, R.J., Dang, S., Yang, R., Li, Q. and Qin, X., 2013. Trends in population blood pressure and prevalence, awareness, treatment, and control of hypertension among middle-aged and older adults in a rural area of Northwest China from 1982 to 2010. *PloS one*, 8(4), pp.e61779.
37. Kearney, P.M., Whelton, M., Reynolds, K., Muntner, P., Whelton, P.K. and He, J., 2005. Global burden of hypertension: analysis of worldwide data. *The Lancet*, 365(9455), pp.217-223.

